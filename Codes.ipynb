{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a158f8ca",
   "metadata": {},
   "source": [
    "# PPO VS DDQN VS DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium import spaces\n",
    "\n",
    " \n",
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, maze_size=8):\n",
    "        super(MazeEnv, self).__init__()\n",
    "        self.maze_size = maze_size\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(maze_size, maze_size), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(4)   \n",
    "        self.maze = None\n",
    "        self.current_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.max_steps = maze_size * maze_size * 2\n",
    "        self.steps = 0\n",
    "        \n",
    "    def generate_maze(self):\n",
    "         \n",
    "        self.maze = np.zeros((self.maze_size, self.maze_size))\n",
    "         \n",
    "        self.maze[np.random.choice(self.maze_size, size=self.maze_size//2), \n",
    "                 np.random.choice(self.maze_size, size=self.maze_size//2)] = 1\n",
    "         \n",
    "        self.current_pos = (0, 0)\n",
    "        self.goal_pos = (self.maze_size-1, self.maze_size-1)\n",
    "        self.maze[self.current_pos] = 0\n",
    "        self.maze[self.goal_pos] = 0\n",
    "        \n",
    "    def get_state(self):\n",
    "        state = self.maze.copy()\n",
    "        state[self.current_pos] = 2   \n",
    "        state[self.goal_pos] = 3      \n",
    "        return state.flatten()\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.generate_maze()\n",
    "        self.steps = 0\n",
    "        return self.get_state(), {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        x, y = self.current_pos\n",
    "        \n",
    "         \n",
    "        if action == 0:     \n",
    "            new_pos = (max(0, x-1), y)\n",
    "        elif action == 1:   \n",
    "            new_pos = (x, min(self.maze_size-1, y+1))\n",
    "        elif action == 2:   \n",
    "            new_pos = (min(self.maze_size-1, x+1), y)\n",
    "        else:              \n",
    "            new_pos = (x, max(0, y-1))\n",
    "            \n",
    "         \n",
    "        if self.maze[new_pos] == 1:\n",
    "            reward = -1   \n",
    "            new_pos = self.current_pos   \n",
    "        else:\n",
    "            self.current_pos = new_pos\n",
    "            \n",
    "         \n",
    "        if self.current_pos == self.goal_pos:\n",
    "            reward = 100   \n",
    "            done = True\n",
    "        elif self.steps >= self.max_steps:\n",
    "            reward = -10   \n",
    "            done = True\n",
    "        else:\n",
    "             \n",
    "            reward = -0.1\n",
    "            done = False\n",
    "            \n",
    "        return self.get_state(), reward, done, done, {}\n",
    "\n",
    " \n",
    "class MazeNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MazeNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    " \n",
    "class MazeDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3):\n",
    "        self.q_network = MazeNetwork(state_dim, action_dim)\n",
    "        self.target_network = MazeNetwork(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.target_update = 10\n",
    "        self.steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(4)   \n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            q_values = self.q_network(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = self.memory.Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state))\n",
    "        action_batch = torch.LongTensor(batch.action)\n",
    "        reward_batch = torch.FloatTensor(batch.reward)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state))\n",
    "        done_batch = torch.FloatTensor(batch.done)\n",
    "\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        next_q_values = self.target_network(next_state_batch).max(1)[0].detach()\n",
    "        target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    " \n",
    "class MazeDDQNAgent(MazeDQNAgent):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3):\n",
    "        super(MazeDDQNAgent, self).__init__(state_dim, action_dim, learning_rate)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = self.memory.Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state))\n",
    "        action_batch = torch.LongTensor(batch.action)\n",
    "        reward_batch = torch.FloatTensor(batch.reward)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state))\n",
    "        done_batch = torch.FloatTensor(batch.done)\n",
    "\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        next_q_values_online = self.q_network(next_state_batch).max(1)[1].detach().unsqueeze(1)\n",
    "        next_q_values_target = self.target_network(next_state_batch).gather(1, next_q_values_online).detach()\n",
    "        target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values_target\n",
    "\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    " \n",
    "class MazePPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3):\n",
    "        self.policy_network = MazeNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_clip = 0.2\n",
    "        self.K = 4   \n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.policy_network(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample().item()\n",
    "        return action\n",
    "\n",
    "    def train(self, memory):\n",
    "        states, actions, rewards, next_states, dones = zip(*memory)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        old_log_probs = torch.FloatTensor([Categorical(logits=self.policy_network(s)).log_prob(a).item() \n",
    "                                           for s, a in zip(states, actions)])\n",
    "\n",
    "        for _ in range(self.K):\n",
    "                    logits = self.policy_network(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "        advantages = rewards + (1 - dones) * self.gamma * self.policy_network(next_states).max(1)[0].detach() - self.policy_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        advantages = advantages.detach()\n",
    "\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    " \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(self.Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train_maze_agent(agent, env, episodes, render=False):\n",
    "    rewards = []\n",
    "    success_rate = []\n",
    "    path_lengths = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        memory = []\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            if hasattr(agent, 'memory'):\n",
    "                agent.memory.push(state, action, reward, next_state, done)\n",
    "                agent.train()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render and episode % 100 == 0:\n",
    "                env.render()\n",
    "\n",
    "        if hasattr(agent, 'train') and not hasattr(agent, 'memory'):\n",
    "            agent.train(memory)\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(steps)\n",
    "        success = reward > 0   \n",
    "        success_rate.append(success)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            recent_success_rate = sum(success_rate[-100:]) / min(100, len(success_rate))\n",
    "            avg_path_length = sum(path_lengths[-100:]) / min(100, len(path_lengths))\n",
    "            print(f\"Episode {episode + 1}, Success Rate: {recent_success_rate:.2%}, \"\n",
    "                  f\"Avg Path Length: {avg_path_length:.1f}, Reward: {total_reward:.2f}\")\n",
    "\n",
    "    return rewards, success_rate, path_lengths\n",
    "\n",
    "def compare_algorithms(env, episodes):\n",
    "    state_dim = env.maze_size * env.maze_size   \n",
    "    action_dim = 4   \n",
    "\n",
    "    agents = {\n",
    "        \"DQN\": MazeDQNAgent(state_dim, action_dim),\n",
    "        \"DDQN\": MazeDDQNAgent(state_dim, action_dim),\n",
    "        \"PPO\": MazePPOAgent(state_dim, action_dim)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for algo_name, agent in agents.items():\n",
    "        print(f\"Training {algo_name}...\")\n",
    "        rewards, success_rate, path_lengths = train_maze_agent(agent, env, episodes)\n",
    "        results[algo_name] = {\n",
    "            \"rewards\": rewards,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"path_lengths\": path_lengths\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    " \n",
    "maze_size = 8\n",
    "env = MazeEnv(maze_size=maze_size)\n",
    "episodes = 100\n",
    "\n",
    "results = compare_algorithms(env, episodes)\n",
    "\n",
    " \n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for algo_name, result in results.items():\n",
    "     \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(result[\"rewards\"], label=f'Total Reward per Episode ({algo_name})')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "     \n",
    "    plt.subplot(1, 3, 2)\n",
    "    window = 100\n",
    "    success_rate_smooth = [sum(result[\"success_rate\"][max(0, i-window):i]) / min(i, window) for i in range(1, len(result[\"success_rate\"]) + 1)]\n",
    "    plt.plot(success_rate_smooth, label=f'Success Rate (Moving Average) ({algo_name})')\n",
    "    plt.title('Success Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "     \n",
    "    plt.subplot(1, 3, 3)\n",
    "    path_lengths_smooth = [sum(result[\"path_lengths\"][max(0, i-window):i]) / min(i, window) for i in range(1, len(result[\"path_lengths\"]) + 1)]\n",
    "    plt.plot(path_lengths_smooth, label=f'Steps per Episode (Moving Average) ({algo_name})')\n",
    "    plt.title('Average Path Length')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773aa9e",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe02234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.x_range = 51\n",
    "        self.y_range = 31\n",
    "        self.motions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                        (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.obs = self.obs_map()\n",
    "        self.state_dim = 14\n",
    "        self.action_dim = len(self.motions)\n",
    "        self.prev_dist = None\n",
    "        self.steps_taken = 0\n",
    "        self.max_steps = 300\n",
    "        self.visited_positions = set()\n",
    "        self.last_positions = []\n",
    "        self.position_history = []\n",
    "        self.stuck_threshold = 20\n",
    "        self.exploration_bonus_map = {}\n",
    "\n",
    "    def obs_map(self):\n",
    "        x = self.x_range\n",
    "        y = self.y_range\n",
    "        obs = set()\n",
    "        for i in range(x):\n",
    "            obs.add((i, 0))\n",
    "        for i in range(x):\n",
    "            obs.add((i, y - 1))\n",
    "        for i in range(y):\n",
    "            obs.add((0, i))\n",
    "        for i in range(y):\n",
    "            obs.add((x - 1, i))\n",
    "        for i in range(10, 21):\n",
    "            obs.add((i, 15))\n",
    "        for i in range(15):\n",
    "            obs.add((20, i))\n",
    "        for i in range(15, 30):\n",
    "            obs.add((30, i))\n",
    "        for i in range(16):\n",
    "            obs.add((40, i))\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = (5, 5)\n",
    "        self.goal = (45, 25)\n",
    "        self.position = self.start\n",
    "        self.prev_dist = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        self.steps_taken = 0\n",
    "        self.visited_positions = {self.start}\n",
    "        self.last_positions = [self.start]\n",
    "        self.position_history = [self.start]\n",
    "        self.exploration_bonus_map = {}\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        pos = np.array(self.position)\n",
    "        goal = np.array(self.goal)\n",
    "        dist_to_goal = np.linalg.norm(pos - goal)\n",
    "        angle_to_goal = np.arctan2(goal[1] - pos[1], goal[0] - pos[0]) / np.pi\n",
    "\n",
    "        obstacle_dists = []\n",
    "        for dx, dy in self.motions:\n",
    "            x, y = self.position\n",
    "            dist = 0\n",
    "            while True:\n",
    "                x += dx\n",
    "                y += dy\n",
    "                dist += 1\n",
    "                if (x, y) in self.obs or not (0 <= x < self.x_range and 0 <= y < self.y_range):\n",
    "                    obstacle_dists.append(min(dist / 10.0, 1.0))\n",
    "                    break\n",
    "\n",
    "        return np.array([\n",
    "            pos[0] / self.x_range,\n",
    "            pos[1] / self.y_range,\n",
    "            goal[0] / self.x_range,\n",
    "            goal[1] / self.y_range,\n",
    "            dist_to_goal / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "            angle_to_goal,\n",
    "            *obstacle_dists\n",
    "        ], dtype=np.float32)\n",
    "    def is_stuck(self):\n",
    "        if len(self.position_history) < self.stuck_threshold:\n",
    "            return False\n",
    "        recent_positions = self.position_history[-self.stuck_threshold:]\n",
    "        unique_positions = set(recent_positions)\n",
    "        return len(unique_positions) <= 3\n",
    "\n",
    "    def get_direction_vector(self, from_pos, to_pos):\n",
    "        vector = np.array(to_pos) - np.array(from_pos)\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / (norm + 1e-6)\n",
    "    def get_exploration_bonus(self, position):\n",
    "         \n",
    "        for pos in self.exploration_bonus_map:\n",
    "            self.exploration_bonus_map[pos] *= 0.995\n",
    "\n",
    "         \n",
    "        if position not in self.exploration_bonus_map:\n",
    "            self.exploration_bonus_map[position] = 10.0   \n",
    "        return self.exploration_bonus_map[position]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_taken += 1\n",
    "        x, y = self.position\n",
    "        dx, dy = self.motions[action]\n",
    "        new_position = (x + dx, y + dy)\n",
    "\n",
    "        current_dist = np.linalg.norm(np.array(new_position) - np.array(self.goal))\n",
    "\n",
    "        if (new_position in self.obs or not (0 <= new_position[0] < self.x_range and\n",
    "                                           0 <= new_position[1] < self.y_range)):\n",
    "            reward = -2.0   \n",
    "            done = False\n",
    "        else:\n",
    "            self.position = new_position\n",
    "            self.visited_positions.add(new_position)\n",
    "            self.last_positions.append(new_position)\n",
    "            self.position_history.append(new_position)\n",
    "            if len(self.last_positions) > 10:\n",
    "                self.last_positions.pop(0)\n",
    "\n",
    "            if self.position == self.goal:\n",
    "                reward = 1000.0   \n",
    "                done = True\n",
    "            else:\n",
    "                progress_reward = (self.prev_dist - current_dist) * 10.0   \n",
    "                distance_factor = 1.0 - (current_dist / np.sqrt(self.x_range**2 + self.y_range**2))\n",
    "                distance_reward = distance_factor * 2.0   \n",
    "\n",
    "\n",
    "                exploration_bonus = self.get_exploration_bonus(new_position)\n",
    "\n",
    "\n",
    "                direction_to_goal = self.get_direction_vector(self.position, self.goal)\n",
    "                movement_direction = self.get_direction_vector((x, y), new_position)\n",
    "                alignment_reward = max(0, np.dot(direction_to_goal, movement_direction)) * 2.0\n",
    "\n",
    "\n",
    "                stuck_penalty = -5.0 if self.is_stuck() else 0.0\n",
    "                oscillation_penalty = -1.0 if new_position in self.last_positions[-3:] else 0.0\n",
    "                time_penalty = -0.1\n",
    "\n",
    "\n",
    "                reward = (progress_reward +\n",
    "                         distance_reward +\n",
    "                         exploration_bonus +\n",
    "                         alignment_reward +\n",
    "                         stuck_penalty +\n",
    "                         oscillation_penalty +\n",
    "                         time_penalty)\n",
    "\n",
    "                 \n",
    "                reward = np.clip(reward, -10.0, 10.0)\n",
    "                done = self.steps_taken >= self.max_steps\n",
    "\n",
    "            self.prev_dist = current_dist\n",
    "\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_probs, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, eps_clip=0.2, K_epochs=8, entropy_coef=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=500, gamma=0.95)\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action_probs, _ = self.policy_old(state)\n",
    "            if not training:\n",
    "                action = torch.argmax(action_probs).item()\n",
    "                return action, None\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def update(self, states, actions, log_probs, rewards, next_states, dones):\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(log_probs).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            action_probs, state_values = self.policy(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            advantages = returns - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * nn.MSELoss()(state_values, returns) - self.entropy_coef * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "def train_agent(env, agent, num_episodes=3000, max_steps=300):\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    best_path = None\n",
    "    best_reward = -float('inf')\n",
    "    last_success_path = None\n",
    "    last_success_reward = None\n",
    "    last_success_length = None\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        path = [env.position]\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            path.append(env.position)\n",
    "\n",
    "            if done:\n",
    "                 \n",
    "                if env.position == env.goal:\n",
    "                    last_success_path = path.copy()\n",
    "                    last_success_reward = total_reward\n",
    "                    last_success_length = step_count\n",
    "                    if total_reward > best_reward:\n",
    "                        best_path = path.copy()\n",
    "                        best_reward = total_reward\n",
    "                break\n",
    "\n",
    "        agent.update(states, actions, log_probs, rewards, next_states, dones)\n",
    "        agent.scheduler.step()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step_count)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {total_reward:.2f}, Steps: {step_count}, LR: {agent.scheduler.get_last_lr()[0]:.6f}\")\n",
    "            visualize_agent_path(env, path, total_reward, step_count)\n",
    "\n",
    "     \n",
    "    print(\"\\nTraining Complete!\")\n",
    "    print(f\"Best path found - Reward: {best_reward:.2f}, Length: {len(best_path)}\")\n",
    "    if best_path:\n",
    "        print(\"Visualizing best path found during training:\")\n",
    "        visualize_agent_path(env, best_path, best_reward, len(best_path))\n",
    "\n",
    "    print(\"\\nLast successful path statistics:\")\n",
    "    if last_success_path:\n",
    "        print(f\"Reward: {last_success_reward:.2f}, Length: {last_success_length}\")\n",
    "        print(\"Visualizing last successful path:\")\n",
    "        visualize_agent_path(env, last_success_path, last_success_reward, last_success_length)\n",
    "    else:\n",
    "        print(\"No successful paths found during training\")\n",
    "\n",
    "    return episode_rewards, episode_lengths, best_path, last_success_path\n",
    "\n",
    "def visualize_agent_path(env, path, episode_reward, episode_length):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    obs_x = [x[0] for x in env.obs]\n",
    "    obs_y = [x[1] for x in env.obs]\n",
    "    plt.plot(obs_x, obs_y, \"sk\", label=\"Obstacles\")\n",
    "    plt.plot(env.start[0], env.start[1], \"bs\", label=\"Start\")\n",
    "    plt.plot(env.goal[0], env.goal[1], \"gs\", label=\"Goal\")\n",
    "    path_x = [p[0] for p in path]\n",
    "    path_y = [p[1] for p in path]\n",
    "    plt.plot(path_x, path_y, 'r-', label=\"Agent Path\")\n",
    "    plt.title(f\"Episode Reward: {episode_reward:.2f}, Length: {episode_length}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = PPOAgent(state_dim=env.state_dim, action_dim=env.action_dim)\n",
    "    episode_rewards, episode_lengths = train_agent(env, agent, num_episodes=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a600319",
   "metadata": {},
   "source": [
    "## WITHOUT FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DualTrainer:\n",
    "    def __init__(self, envs, episodes=1000):\n",
    "        self.envs = envs\n",
    "        self.episodes = episodes\n",
    "        self.agents = [\n",
    "            PPOAgent(env.state_dim, env.action_dim) \n",
    "            for env in envs\n",
    "        ]\n",
    "        self.rewards_history = [[] for _ in envs]\n",
    "        self.best_paths = [None] * len(envs)\n",
    "        self.best_rewards = [-float('inf')] * len(envs)\n",
    "        self.current_paths = [[] for _ in envs]\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting Dual PPO Training\")\n",
    "        for episode in range(self.episodes):\n",
    "             \n",
    "            for agent_id in range(len(self.agents)):\n",
    "                state = self.envs[agent_id].reset()\n",
    "                current_path = [self.envs[agent_id].position]\n",
    "                \n",
    "                states, actions, log_probs, rewards = [], [], [], []\n",
    "                next_states, dones = [], []\n",
    "                episode_reward = 0\n",
    "                \n",
    "                for step in range(self.envs[agent_id].max_steps):\n",
    "                    action, log_prob = self.agents[agent_id].select_action(state)\n",
    "                    next_state, reward, done, _ = self.envs[agent_id].step(action)\n",
    "                    \n",
    "                    states.append(state)\n",
    "                    actions.append(action)\n",
    "                    log_probs.append(log_prob)\n",
    "                    rewards.append(reward)\n",
    "                    next_states.append(next_state)\n",
    "                    dones.append(done)\n",
    "                    current_path.append(self.envs[agent_id].position)\n",
    "                    \n",
    "                    state = next_state\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done:\n",
    "                        self.current_paths[agent_id] = current_path\n",
    "                        if self.envs[agent_id].position == self.envs[agent_id].goal:\n",
    "                            if episode_reward > self.best_rewards[agent_id]:\n",
    "                                self.best_paths[agent_id] = current_path.copy()\n",
    "                                self.best_rewards[agent_id] = episode_reward\n",
    "                                print(f\"New best path found for Agent {agent_id + 1}! Reward: {episode_reward:.2f}\")\n",
    "                        break\n",
    "                \n",
    "                self.rewards_history[agent_id].append(episode_reward)\n",
    "                self.agents[agent_id].update(states, actions, log_probs, rewards, next_states, dones)\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"\\nEpisode {episode + 1}\")\n",
    "                for agent_id in range(len(self.agents)):\n",
    "                    avg_reward = np.mean(self.rewards_history[agent_id][-100:])\n",
    "                    print(f\"Agent {agent_id + 1} Average Reward: {avg_reward:.2f}\")\n",
    "                self.visualize_current_paths(episode + 1)\n",
    "\n",
    "         \n",
    "        self.visualize_final_results()\n",
    "\n",
    "    def visualize_current_paths(self, episode):\n",
    "        \"\"\"Show current paths for both agents\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        fig.suptitle(f'Current Paths - Episode {episode}')\n",
    "        \n",
    "        for i in range(len(self.agents)):\n",
    "            ax = axes[i]\n",
    "            env = self.envs[i]\n",
    "            \n",
    "             \n",
    "            self._plot_environment(ax, env)\n",
    "            \n",
    "             \n",
    "            if self.current_paths[i]:\n",
    "                path_x = [p[0] for p in self.current_paths[i]]\n",
    "                path_y = [p[1] for p in self.current_paths[i]]\n",
    "                ax.plot(path_x, path_y, 'b-', label=\"Current Path\", linewidth=2)\n",
    "            \n",
    "            ax.set_title(f'Agent {i+1}\\nCurrent Reward: {self.rewards_history[i][-1]:.2f}')\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            ax.axis(\"equal\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_final_results(self):\n",
    "        \"\"\"Show final results with best paths and learning curves\"\"\"\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        gs = plt.GridSpec(2, 2, figure=fig)\n",
    "        \n",
    "         \n",
    "        for i in range(len(self.agents)):\n",
    "            ax_path = fig.add_subplot(gs[0, i])\n",
    "            env = self.envs[i]\n",
    "            \n",
    "             \n",
    "            self._plot_environment(ax_path, env)\n",
    "            \n",
    "             \n",
    "            if self.best_paths[i]:\n",
    "                path_x = [p[0] for p in self.best_paths[i]]\n",
    "                path_y = [p[1] for p in self.best_paths[i]]\n",
    "                ax_path.plot(path_x, path_y, 'r-', label=\"Best Path\", linewidth=2)\n",
    "            \n",
    "            ax_path.set_title(f'Agent {i+1} Best Path\\nBest Reward: {self.best_rewards[i]:.2f}')\n",
    "            ax_path.legend()\n",
    "            ax_path.grid(True)\n",
    "            ax_path.axis(\"equal\")\n",
    "        \n",
    "         \n",
    "        ax_metrics = fig.add_subplot(gs[1, :])\n",
    "        for i, rewards in enumerate(self.rewards_history):\n",
    "             \n",
    "            window = min(100, len(rewards))\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax_metrics.plot(range(window-1, len(rewards)), smoothed, \n",
    "                          linewidth=2, label=f'Agent {i+1}')\n",
    "        \n",
    "        ax_metrics.set_title('Training Progress')\n",
    "        ax_metrics.set_xlabel('Episode')\n",
    "        ax_metrics.set_ylabel('Average Reward (100-episode window)')\n",
    "        ax_metrics.legend()\n",
    "        ax_metrics.grid(True)\n",
    "        \n",
    "        plt.suptitle('Final Training Results')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_environment(self, ax, env):\n",
    "        \"\"\"Helper method to plot the environment elements\"\"\"\n",
    "         \n",
    "        if hasattr(env, 'obs') and env.obs:\n",
    "            obs_x = [x[0] for x in env.obs]\n",
    "            obs_y = [x[1] for x in env.obs]\n",
    "            ax.scatter(obs_x, obs_y, c='black', marker='s', s=100, label=\"Obstacles\")\n",
    "        \n",
    "         \n",
    "        ax.scatter(env.start[0], env.start[1], c='blue', marker='o', s=100, label=\"Start\")\n",
    "        ax.scatter(env.goal[0], env.goal[1], c='green', marker='*', s=200, label=\"Goal\")\n",
    "        \n",
    "         \n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "         \n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "def train_dual_ppo(episodes=1000):\n",
    "    \"\"\"Main function to train two independent PPO agents\"\"\"\n",
    "    envs = [Env(), Env()]\n",
    "    trainer = DualTrainer(envs, episodes=episodes)\n",
    "    trainer.train()\n",
    "    return trainer.best_paths, trainer.best_rewards\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_paths, best_rewards = train_dual_ppo(episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32ac71",
   "metadata": {},
   "source": [
    "## WITH FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a666b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.distributions import Categorical\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FederatedPPO:\n",
    "    def __init__(self, num_clients=2, state_dim=14, action_dim=8):\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.global_model = ActorCritic(state_dim, action_dim).to(device)\n",
    "\n",
    "         \n",
    "        for _ in range(num_clients):\n",
    "            client = PPOAgent(state_dim, action_dim)\n",
    "            self.clients.append(client)\n",
    "\n",
    "    def federated_average(self, weight_accumulator):\n",
    "        averaged_weights = {}\n",
    "        for name, param in self.global_model.state_dict().items():\n",
    "            averaged_weights[name] = torch.zeros_like(param)\n",
    "\n",
    "        total_samples = sum(accumulator['n_samples'] for accumulator in weight_accumulator.values())\n",
    "\n",
    "        for client_id, accumulator in weight_accumulator.items():\n",
    "            weight = accumulator['n_samples'] / total_samples\n",
    "            for name, diff in accumulator['weights'].items():\n",
    "                averaged_weights[name] += weight * diff\n",
    "\n",
    "        return averaged_weights\n",
    "\n",
    "    def update_global_model(self, weight_accumulator):\n",
    "        averaged_weights = self.federated_average(weight_accumulator)\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.global_model.named_parameters():\n",
    "                param.add_(averaged_weights[name])\n",
    "\n",
    "    def distribute_global_model(self):\n",
    "        for client in self.clients:\n",
    "            client.policy.load_state_dict(self.global_model.state_dict())\n",
    "            client.policy_old.load_state_dict(self.global_model.state_dict())\n",
    "\n",
    "class FederatedTrainer:\n",
    "    def __init__(self, federated_ppo, envs, rounds=30, local_episodes=50):\n",
    "        self.federated_ppo = federated_ppo\n",
    "        self.envs = envs\n",
    "        self.rounds = rounds\n",
    "        self.local_episodes = local_episodes\n",
    "        self.global_rewards = []\n",
    "        self.client_rewards = [[] for _ in range(len(envs))]\n",
    "        self.best_path = None\n",
    "        self.best_reward = -float('inf')\n",
    "        self.client_best_paths = [None] * len(envs)\n",
    "        self.client_best_rewards = [-float('inf')] * len(envs)\n",
    "\n",
    "    def compute_weight_update(self, client_id, old_model, new_model):\n",
    "        weight_diff = {}\n",
    "        for name, old_param in old_model.state_dict().items():\n",
    "            new_param = new_model.state_dict()[name]\n",
    "            weight_diff[name] = new_param - old_param\n",
    "        return weight_diff\n",
    "\n",
    "    def train_client(self, client_id, env):\n",
    "        client = self.federated_ppo.clients[client_id]\n",
    "        total_samples = 0\n",
    "        episode_rewards = []\n",
    "        best_local_path = None\n",
    "        best_local_reward = -float('inf')\n",
    "\n",
    "        for episode in range(self.local_episodes):\n",
    "            state = env.reset()\n",
    "            path = [env.position]\n",
    "\n",
    "            states, actions, log_probs, rewards = [], [], [], []\n",
    "            next_states, dones = [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(env.max_steps):\n",
    "                action, log_prob = client.select_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                path.append(env.position)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    if env.position == env.goal and episode_reward > best_local_reward:\n",
    "                        best_local_path = path.copy()\n",
    "                        best_local_reward = episode_reward\n",
    "                    break\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "            client.update(states, actions, log_probs, rewards, next_states, dones)\n",
    "            total_samples += len(states)\n",
    "\n",
    "        return total_samples, best_local_path, best_local_reward, np.mean(episode_rewards)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting Federated Training with 2 Clients\")\n",
    "        for round_num in range(self.rounds):\n",
    "            print(f\"\\nFederated Round {round_num + 1}/{self.rounds}\")\n",
    "            weight_accumulator = {}\n",
    "            round_rewards = []\n",
    "\n",
    "             \n",
    "            global_model_state = copy.deepcopy(self.federated_ppo.global_model.state_dict())\n",
    "\n",
    "             \n",
    "            for client_id in range(self.federated_ppo.num_clients):\n",
    "                print(f\"Training Client {client_id + 1}\")\n",
    "                n_samples, local_best_path, local_best_reward, avg_reward = self.train_client(\n",
    "                    client_id, self.envs[client_id]\n",
    "                )\n",
    "\n",
    "                self.client_rewards[client_id].append(avg_reward)\n",
    "                round_rewards.append(avg_reward)\n",
    "\n",
    "                 \n",
    "                if local_best_reward > self.client_best_rewards[client_id]:\n",
    "                    self.client_best_paths[client_id] = local_best_path\n",
    "                    self.client_best_rewards[client_id] = local_best_reward\n",
    "\n",
    "                weight_diff = self.compute_weight_update(\n",
    "                    client_id,\n",
    "                    self.federated_ppo.global_model,\n",
    "                    self.federated_ppo.clients[client_id].policy\n",
    "                )\n",
    "\n",
    "                weight_accumulator[client_id] = {\n",
    "                    'weights': weight_diff,\n",
    "                    'n_samples': n_samples\n",
    "                }\n",
    "\n",
    "             \n",
    "            self.federated_ppo.update_global_model(weight_accumulator)\n",
    "            self.federated_ppo.distribute_global_model()\n",
    "\n",
    "             \n",
    "            avg_round_reward = np.mean(round_rewards)\n",
    "            self.global_rewards.append(avg_round_reward)\n",
    "\n",
    "            print(f\"Round {round_num + 1} Summary:\")\n",
    "            print(f\"Global Average Reward: {avg_round_reward:.2f}\")\n",
    "            for client_id in range(self.federated_ppo.num_clients):\n",
    "                print(f\"Client {client_id + 1} Average Reward: {round_rewards[client_id]:.2f}\")\n",
    "\n",
    "             \n",
    "            if (round_num + 1) % 5 == 0:\n",
    "                self.visualize_all_paths(round_num + 1)\n",
    "\n",
    "    def visualize_all_paths(self, round_num):\n",
    "        \"\"\"Visualize paths for both clients and global model side by side\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        fig.suptitle(f'Best Paths after Round {round_num}')\n",
    "\n",
    "        for i, (path, reward) in enumerate(zip(self.client_best_paths, self.client_best_rewards)):\n",
    "            if path:\n",
    "                ax = axes[i]\n",
    "                env = self.envs[i]\n",
    "\n",
    "                 \n",
    "                obs_x = [x[0] for x in env.obs]\n",
    "                obs_y = [x[1] for x in env.obs]\n",
    "                ax.plot(obs_x, obs_y, \"sk\", label=\"Obstacles\")\n",
    "\n",
    "                 \n",
    "                ax.plot(env.start[0], env.start[1], \"bs\", label=\"Start\")\n",
    "                ax.plot(env.goal[0], env.goal[1], \"gs\", label=\"Goal\")\n",
    "\n",
    "                 \n",
    "                path_x = [p[0] for p in path]\n",
    "                path_y = [p[1] for p in path]\n",
    "                ax.plot(path_x, path_y, 'r-', label=\"Path\")\n",
    "\n",
    "                ax.set_title(f'Client {i+1}\\nReward: {reward:.2f}')\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "                ax.axis(\"equal\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Plot training progress for global model and both clients\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "         \n",
    "        plt.plot(self.global_rewards, 'k-', label='Global Average', linewidth=2)\n",
    "\n",
    "         \n",
    "        for i, rewards in enumerate(self.client_rewards):\n",
    "            plt.plot(rewards, '--', label=f'Client {i+1}', alpha=0.7)\n",
    "\n",
    "        plt.title('Training Progress')\n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def train_federated_ppo(rounds=30, local_episodes=50):\n",
    "    \"\"\"Main function to train 2-client federated PPO\"\"\"\n",
    "     \n",
    "    envs = [Env(), Env()]\n",
    "\n",
    "     \n",
    "    fed_ppo = FederatedPPO(\n",
    "        num_clients=2,\n",
    "        state_dim=envs[0].state_dim,\n",
    "        action_dim=envs[0].action_dim\n",
    "    )\n",
    "\n",
    "     \n",
    "    trainer = FederatedTrainer(\n",
    "        federated_ppo=fed_ppo,\n",
    "        envs=envs,\n",
    "        rounds=rounds,\n",
    "        local_episodes=local_episodes\n",
    "    )\n",
    "\n",
    "     \n",
    "    trainer.train()\n",
    "\n",
    "     \n",
    "    trainer.plot_training_progress()\n",
    "\n",
    "    return trainer.client_best_paths, trainer.client_best_rewards\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    best_paths, best_rewards = train_federated_ppo(\n",
    "        rounds=30,\n",
    "        local_episodes=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd4549",
   "metadata": {},
   "source": [
    "# MULTI-AGENT IN STATIC OBSTACLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CBSConstraints(object):\n",
    "    def __init__(self):\n",
    "        self.vertex_constraints = set()\n",
    "        self.edge_constraints = set()\n",
    "\n",
    "    def add_constraint(self, other):\n",
    "        self.vertex_constraints |= other.vertex_constraints\n",
    "        self.edge_constraints |= other.edge_constraints\n",
    "\n",
    "class Location(object):\n",
    "    def __init__(self, x=-1, y=-1):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    def __str__(self):\n",
    "        return str((self.x, self.y))\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, time, location):\n",
    "        self.time = time\n",
    "        self.location = location\n",
    "    def __eq__(self, other):\n",
    "        return self.time == other.time and self.location == other.location\n",
    "    def __hash__(self):\n",
    "        return hash((self.time, self.location.x, self.location.y))\n",
    "    def is_equal_except_time(self, state):\n",
    "        return self.location == state.location\n",
    "    def __str__(self):\n",
    "        return f\"({self.time}, {self.location.x}, {self.location.y})\"\n",
    "\n",
    "class Conflict(object):\n",
    "    VERTEX = 1\n",
    "    EDGE = 2\n",
    "    def __init__(self):\n",
    "        self.time = -1\n",
    "        self.type = -1\n",
    "        self.agent_1 = ''\n",
    "        self.agent_2 = ''\n",
    "        self.location_1 = Location()\n",
    "        self.location_2 = Location()\n",
    "    def __str__(self):\n",
    "        return f\"({self.time}, {self.agent_1}, {self.agent_2}, {self.location_1}, {self.location_2})\"\n",
    "\n",
    "def get_state(solution, agent_name, t):\n",
    "    if t < len(solution[agent_name]):\n",
    "        return solution[agent_name][t]\n",
    "    else:\n",
    "        return solution[agent_name][-1]\n",
    "\n",
    "def detect_first_conflict(solution):\n",
    "    max_t = max(len(path) for path in solution.values())\n",
    "    for t in range(max_t):\n",
    "        for agent1, agent2 in combinations(solution.keys(), 2):\n",
    "            state1 = get_state(solution, agent1, t)\n",
    "            state2 = get_state(solution, agent2, t)\n",
    "            if state1.is_equal_except_time(state2):\n",
    "                conflict = Conflict()\n",
    "                conflict.time = t\n",
    "                conflict.type = Conflict.VERTEX\n",
    "                conflict.agent_1 = agent1\n",
    "                conflict.agent_2 = agent2\n",
    "                conflict.location_1 = state1.location\n",
    "                return conflict\n",
    "        for agent1, agent2 in combinations(solution.keys(), 2):\n",
    "            state1a = get_state(solution, agent1, t)\n",
    "            state1b = get_state(solution, agent1, t+1)\n",
    "            state2a = get_state(solution, agent2, t)\n",
    "            state2b = get_state(solution, agent2, t+1)\n",
    "            if state1a.is_equal_except_time(state2b) and state1b.is_equal_except_time(state2a):\n",
    "                conflict = Conflict()\n",
    "                conflict.time = t\n",
    "                conflict.type = Conflict.EDGE\n",
    "                conflict.agent_1 = agent1\n",
    "                conflict.agent_2 = agent2\n",
    "                conflict.location_1 = state1a.location\n",
    "                conflict.location_2 = state1b.location\n",
    "                return conflict\n",
    "    return None\n",
    "\n",
    "def detect_conflict(paths):\n",
    "    solution = {}\n",
    "    for i, path in enumerate(paths):\n",
    "        agent_name = f\"agent{i}\"\n",
    "        state_list = []\n",
    "        for t, pos in enumerate(path):\n",
    "            state_list.append(State(t, Location(pos[0], pos[1])))\n",
    "        solution[agent_name] = state_list\n",
    "    return detect_first_conflict(solution)\n",
    "\n",
    "class VertexConstraint(object):\n",
    "    def __init__(self, time, location):\n",
    "        self.time = time\n",
    "        self.location = location\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.time == other.time and self.location == other.location\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.time) + str(self.location))\n",
    "\n",
    "class EdgeConstraint(object):\n",
    "    def __init__(self, time, location_1, location_2):\n",
    "        self.time = time\n",
    "        self.location_1 = location_1\n",
    "        self.location_2 = location_2\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return (self.time == other.time and \n",
    "                self.location_1 == other.location_1 and \n",
    "                self.location_2 == other.location_2)\n",
    "                \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.time) + str(self.location_1) + str(self.location_2))\n",
    "\n",
    "class CBSNode:\n",
    "    def __init__(self):\n",
    "        self.solution = {}\n",
    "        self.constraints = {}\n",
    "        self.cost = 0\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.cost < other.cost\n",
    "\n",
    "def compute_potential_field(pos, other_pos, other_path, goal_pos, obs, x_range, y_range):\n",
    "    pos = np.array(pos)\n",
    "    other_pos = np.array(other_pos)\n",
    "    goal_pos = np.array(goal_pos)\n",
    "\n",
    "    k_attr = 1.0   \n",
    "    k_rep = 150.0   \n",
    "    k_obs = 100.0   \n",
    "    k_path = 100.0  \n",
    "    d0 = 7.0       \n",
    "    d_path = 5.0   \n",
    "\n",
    "    f_attr = k_attr * (goal_pos - pos)\n",
    "    \n",
    "    dist_to_other = np.linalg.norm(pos - other_pos)\n",
    "    if dist_to_other < d0:\n",
    "        f_rep = k_rep * (1/dist_to_other - 1/d0) * (1/dist_to_other**2) * (pos - other_pos)\n",
    "    else:\n",
    "        f_rep = np.zeros(2)\n",
    "\n",
    "    f_path = np.zeros(2)\n",
    "    if other_path:\n",
    "        for path_pos in other_path:\n",
    "            path_pos = np.array(path_pos)\n",
    "            dist_to_path = np.linalg.norm(pos - path_pos)\n",
    "            if dist_to_path < d_path:\n",
    "                f_path += k_path * (1/dist_to_path - 1/d_path) * (1/dist_to_path**2) * (pos - path_pos)\n",
    "\n",
    "    f_obs = np.zeros(2)\n",
    "    for dx, dy in [(0,1), (1,0), (0,-1), (-1,0), (1,1), (-1,1), (1,-1), (-1,-1)]:\n",
    "        check_pos = (int(pos[0] + dx), int(pos[1] + dy))\n",
    "        if (check_pos in obs or \n",
    "            not (0 <= check_pos[0] < x_range and 0 <= check_pos[1] < y_range) or\n",
    "            check_pos in other_path):\n",
    "            dist = np.linalg.norm(np.array([dx, dy]))\n",
    "            if dist < d0:\n",
    "                f_obs += k_obs * (1/dist - 1/d0) * (1/dist**2) * np.array([-dx, -dy])\n",
    "\n",
    "    f_total = f_attr + 2*f_rep + f_obs + 1.5*f_path\n",
    "\n",
    "    f_norm = np.linalg.norm(f_total)\n",
    "    if f_norm > 0:\n",
    "        f_total = f_total / f_norm\n",
    "        \n",
    "    return f_total\n",
    "\n",
    "class MultiAgentEnv:\n",
    "    def __init__(self):\n",
    "        self.x_range = 61\n",
    "        self.y_range = 41\n",
    "        self.motions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                        (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.obs = self.obs_map()\n",
    "        self.state_dim = 23\n",
    "        self.action_dim = len(self.motions)\n",
    "        self.max_steps = 500\n",
    "        self.cbs_constraints = CBSConstraints()\n",
    "        self.agent_paths = [[], []]\n",
    "\n",
    "        self.safety_radius = 3        \n",
    "        self.collision_penalty = -40       \n",
    "        self.path_penalty = -20            \n",
    "        self.min_path_distance = 2         \n",
    "        self.path_memory_length = 5       \n",
    "\n",
    "        self.stagnation_penalty = -400     \n",
    "        \n",
    "        self.goal_reached_bonus = 2000.0\n",
    "        self.progress_reward_scale = 30.0  \n",
    "\n",
    "        self.agents = [\n",
    "            {'start': (10, 30), 'goal': (55, 5), 'position': (10, 30), 'steps_taken': 0},\n",
    "            {'start': (5, 5), 'goal': (45, 35), 'position': (5, 5), 'steps_taken': 0}\n",
    "        ]\n",
    "        \n",
    "        self.path_crossing_cooldown = 20  \n",
    "        self.last_crossing = [0, 0]       \n",
    "        self.alternative_path_bonus = 15   \n",
    "        self.deadlock_threshold = 15      \n",
    "        self.deadlock_counter = [0, 0]\n",
    "        self.deadlock_counter = [0, 0] \n",
    "        self.path_history = [[] for _ in range(len(self.agents))]  \n",
    "        self.path_obstacle_memory = 5 \n",
    "    def apply_path_constraints(self):\n",
    "        for t in range(len(self.agent_paths[0])):\n",
    "            for i in range(len(self.agent_paths)):\n",
    "                if t < len(self.agent_paths[i]):\n",
    "                    loc = Location(self.agent_paths[i][t][0], self.agent_paths[i][t][1])\n",
    "                    vertex_const = VertexConstraint(t, loc)\n",
    "                    self.cbs_constraints.vertex_constraints.add(vertex_const)\n",
    "\n",
    "    def obs_map(self):\n",
    "        x = self.x_range\n",
    "        y = self.y_range\n",
    "        obs = set()\n",
    "        for i in range(x):\n",
    "            obs.add((i, 0))\n",
    "        for i in range(x):\n",
    "            obs.add((i, y - 1))\n",
    "        for i in range(y):\n",
    "            obs.add((0, i))\n",
    "        for i in range(y):\n",
    "            obs.add((x - 1, i))\n",
    "        for i in range(10, 21):\n",
    "            obs.add((i, 15))\n",
    "        for i in range(15):\n",
    "            obs.add((20, i))\n",
    "        for i in range(15, 30):\n",
    "            obs.add((30, i))\n",
    "        for i in range(16):\n",
    "            obs.add((40, i))\n",
    "        num_random_obstacles = 30\n",
    "        for _ in range(num_random_obstacles):\n",
    "            rx = np.random.randint(1, self.x_range - 1)\n",
    "            ry = np.random.randint(1, self.y_range - 1)\n",
    "            if (rx, ry) in [(5, 5), (55, 35), (55, 5), (10, 30)]:\n",
    "                continue\n",
    "            obs.add((rx, ry))\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_paths = [[], []]\n",
    "        \n",
    "        self.agents = [\n",
    "            {'start': (10, 30), 'goal': (55, 5), 'position': (10, 30), 'steps_taken': 0},\n",
    "            {'start': (5, 5), 'goal': (45, 35), 'position': (5, 5), 'steps_taken': 0}\n",
    "        ]\n",
    "        self.visited_positions = [{agent['start']} for agent in self.agents]\n",
    "        self.exploration_bonuses = [{} for _ in range(2)]\n",
    "        self.stuck_counters = [0, 0]\n",
    "        self.prev_positions = [[], []]\n",
    "        for agent in self.agents:\n",
    "            start = np.array(agent['start'])\n",
    "            goal = np.array(agent['goal'])\n",
    "            agent['prev_dist'] = np.linalg.norm(start - goal)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        states = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            pos = np.array(agent['position'])\n",
    "            goal = np.array(agent['goal'])\n",
    "            other_agent_pos = np.array(self.agents[1-i]['position'])\n",
    "            other_agent_path = self.agent_paths[1-i]\n",
    "            \n",
    "            min_path_dist = float('inf')\n",
    "            if other_agent_path:\n",
    "                for path_pos in other_agent_path:\n",
    "                    dist = np.linalg.norm(np.array(path_pos) - pos)\n",
    "                    min_path_dist = min(min_path_dist, dist)\n",
    "            min_path_dist = min(min_path_dist, np.sqrt(self.x_range**2 + self.y_range**2))\n",
    "            \n",
    "            dist_to_goal = np.linalg.norm(pos - goal)\n",
    "            dist_to_other_agent = np.linalg.norm(pos - other_agent_pos)\n",
    "            angle_to_goal = np.arctan2(goal[1]-pos[1], goal[0]-pos[0]) / np.pi\n",
    "            angle_to_other_agent = np.arctan2(other_agent_pos[1]-pos[1], other_agent_pos[0]-pos[0]) / np.pi\n",
    "            \n",
    "            other_agent_prev_pos = np.array(self.agents[1-i].get('prev_position', other_agent_pos))\n",
    "            relative_velocity = (other_agent_pos - other_agent_prev_pos)\n",
    "            rel_vel_x = relative_velocity[0] / self.x_range\n",
    "            rel_vel_y = relative_velocity[1] / self.y_range\n",
    "            \n",
    "            if np.any(relative_velocity):\n",
    "                time_to_collision = dist_to_other_agent / np.linalg.norm(relative_velocity)\n",
    "                time_to_collision = np.clip(time_to_collision / 10.0, 0, 1)\n",
    "            else:\n",
    "                time_to_collision = 1.0\n",
    "\n",
    "            obstacle_dists = []\n",
    "            for dx, dy in self.motions:\n",
    "                x, y = agent['position']\n",
    "                dist = 0\n",
    "                while True:\n",
    "                    x += dx\n",
    "                    y += dy\n",
    "                    dist += 1\n",
    "                    next_pos = (x, y)\n",
    "                    if (next_pos in self.obs or \n",
    "                        not (0 <= x < self.x_range and 0 <= y < self.y_range) or\n",
    "                        next_pos in other_agent_path or\n",
    "                        (np.linalg.norm(np.array([x, y]) - other_agent_pos) < self.safety_radius)):\n",
    "                        obstacle_dists.append(min(dist/10.0, 1.0))\n",
    "                        break\n",
    "\n",
    "            norm_path_dist = min_path_dist / np.sqrt(self.x_range**2 + self.y_range**2)\n",
    "            \n",
    "            state = np.array([\n",
    "                pos[0] / self.x_range,\n",
    "                pos[1] / self.y_range,\n",
    "                goal[0] / self.x_range,\n",
    "                goal[1] / self.y_range,\n",
    "                dist_to_goal / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "                angle_to_goal,\n",
    "                other_agent_pos[0] / self.x_range,\n",
    "                other_agent_pos[1] / self.y_range,\n",
    "                dist_to_other_agent / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "                angle_to_other_agent,\n",
    "                rel_vel_x,\n",
    "                rel_vel_y,\n",
    "                time_to_collision,\n",
    "                norm_path_dist,  \n",
    "                *obstacle_dists,  \n",
    "                agent['steps_taken'] / self.max_steps\n",
    "            ], dtype=np.float32)\n",
    "            states.append(state)\n",
    "        return states\n",
    "\n",
    "    def get_exploration_bonus(self, agent_idx, position):\n",
    "        if position not in self.exploration_bonuses[agent_idx]:\n",
    "            self.exploration_bonuses[agent_idx][position] = 1.0\n",
    "        else:\n",
    "            self.exploration_bonuses[agent_idx][position] *= self.exploration_decay\n",
    "            self.exploration_bonuses[agent_idx][position] = max(\n",
    "                self.exploration_bonuses[agent_idx][position],\n",
    "                self.min_exploration_bonus\n",
    "            )\n",
    "        return self.exploration_bonuses[agent_idx][position]\n",
    "\n",
    "    def validate_move(self, pos, time, agent_id):\n",
    "        x, y = pos\n",
    "        if not (0 <= x < self.x_range and 0 <= y < self.y_range):\n",
    "            return False\n",
    "        if pos in self.obs:\n",
    "            return False\n",
    "            \n",
    "        other_agent_path = self.agent_paths[1-agent_id]\n",
    "        other_agent_pos = self.agents[1-agent_id]['position']\n",
    "        pos_array = np.array(pos)\n",
    "        safety_distance = 4.0\n",
    "        \n",
    "        other_pos_array = np.array(other_agent_pos)\n",
    "        if np.linalg.norm(pos_array - other_pos_array) < safety_distance:\n",
    "            return False\n",
    "            \n",
    "        if other_agent_path:\n",
    "            for path_pos in other_agent_path:\n",
    "                path_pos_array = np.array(path_pos)\n",
    "                dist = np.linalg.norm(pos_array - path_pos_array)\n",
    "                if dist < safety_distance:\n",
    "                    return False\n",
    "        \n",
    "        if len(other_agent_path) >= 2:\n",
    "            current_pos = np.array(self.agents[agent_id]['position'])\n",
    "            \n",
    "            for i in range(len(other_agent_path) - 1):\n",
    "                path_seg_start = np.array(other_agent_path[i])\n",
    "                path_seg_end = np.array(other_agent_path[i + 1])\n",
    "                corridor_width = 5.0  \n",
    "                path_vector = path_seg_end - path_seg_start\n",
    "                path_length = np.linalg.norm(path_vector)\n",
    "                if path_length == 0:\n",
    "                    continue\n",
    "                    \n",
    "                perp_vector = np.array([-path_vector[1], path_vector[0]]) / path_length\n",
    "                \n",
    "                pos_relative = pos_array - path_seg_start\n",
    "                dist_from_path = abs(np.dot(pos_relative, perp_vector))\n",
    "                \n",
    "                if dist_from_path < corridor_width:\n",
    "                    proj_length = np.dot(pos_relative, path_vector) / path_length\n",
    "                    if 0 <= proj_length <= path_length:\n",
    "                        return False\n",
    "    \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i != agent_id and pos == agent['goal'] and pos == agent['start']:\n",
    "                return False\n",
    "                \n",
    "        loc = Location(pos[0], pos[1])\n",
    "        vertex_const = VertexConstraint(time, loc)\n",
    "        return vertex_const not in self.cbs_constraints.vertex_constraints\n",
    "        \n",
    "    def validate_transition(self, pos1, pos2,time):\n",
    "        loc1 = Location(pos1[0], pos1[1])\n",
    "        loc2 = Location(pos2[0], pos2[1])\n",
    "        edge_const = EdgeConstraint(time, loc1, loc2)\n",
    "        return edge_const not in self.cbs_constraints.edge_constraints\n",
    "\n",
    "    def check_stuck(self, agent_idx, position):\n",
    "        self.prev_positions[agent_idx].append(position)\n",
    "        if len(self.prev_positions[agent_idx]) > 10:\n",
    "            self.prev_positions[agent_idx].pop(0)\n",
    "        if len(self.prev_positions[agent_idx]) == 10:\n",
    "            unique_positions = len(set(self.prev_positions[agent_idx]))\n",
    "            if unique_positions <= 3:\n",
    "                self.stuck_counters[agent_idx] += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = [0, 0]\n",
    "        dones = [False, False]\n",
    "        prev_positions = [agent['position'] for agent in self.agents]\n",
    "        prev_distances = [agent['prev_dist'] for agent in self.agents]\n",
    "        current_time = self.agents[0]['steps_taken']\n",
    "    \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            position = agent['position']\n",
    "            self.path_history[i].append(position)\n",
    "    \n",
    "            if len(self.path_history[i]) > self.path_obstacle_memory:\n",
    "                self.path_history[i].pop(0)\n",
    "        candidate_positions = []\n",
    "        for i, (agent, action) in enumerate(zip(self.agents, actions)):\n",
    "            if agent.get('reached_goal', False):\n",
    "                candidate_positions.append(agent['position'])\n",
    "                continue\n",
    "                \n",
    "            x, y = agent['position']\n",
    "            dx, dy = self.motions[action]\n",
    "            new_pos = (x+dx, y+dy)\n",
    "            \n",
    "            if self.check_deadlock(i, new_pos):\n",
    "                self.deadlock_counter[i] += 1\n",
    "            else:\n",
    "                self.deadlock_counter[i] = 0\n",
    "            \n",
    "            path_violation = self.check_path_violation(i, new_pos,i)\n",
    "            other_agent_dist = np.linalg.norm(np.array(new_pos) - np.array(self.agents[1-i]['position']))\n",
    "            \n",
    "            if path_violation:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.path_penalty * 0.5\n",
    "            elif new_pos == self.agents[1-i]['position']:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.collision_penalty\n",
    "            elif other_agent_dist < self.safety_radius:\n",
    "                current_dist_to_goal = np.linalg.norm(np.array(new_pos) - np.array(agent['goal']))\n",
    "                if current_dist_to_goal < agent['prev_dist']:\n",
    "                    candidate_positions.append(new_pos)\n",
    "                    rewards[i] += self.collision_penalty * 0.2\n",
    "                else:\n",
    "                    candidate_positions.append(agent['position'])\n",
    "                    rewards[i] += self.collision_penalty * 0.3\n",
    "            elif self.validate_move(new_pos, current_time + 1, i):\n",
    "                candidate_positions.append(new_pos)\n",
    "                if self.is_alternative_path(i, new_pos):\n",
    "                    rewards[i] += self.alternative_path_bonus\n",
    "            else:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.collision_penalty * 0.2\n",
    "    \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if agent.get('reached_goal', False):\n",
    "                continue\n",
    "                \n",
    "            new_position = candidate_positions[i]\n",
    "            old_position = agent['position']\n",
    "            agent['position'] = new_position\n",
    "            agent['steps_taken'] += 1\n",
    "            \n",
    "            if self.deadlock_counter[i] >= self.deadlock_threshold:\n",
    "                self.min_path_distance = max(1, self.min_path_distance - 0.5)\n",
    "                rewards[i] += self.stagnation_penalty * 5\n",
    "            else:\n",
    "                self.min_path_distance = min(2, self.min_path_distance + 0.1)\n",
    "            \n",
    "            self.update_path_history(i, new_position)\n",
    "            \n",
    "            rewards[i] += self.compute_rewards(i, new_position, old_position)\n",
    "            \n",
    "            if new_position == agent['goal']:\n",
    "                rewards[i] += self.goal_reached_bonus\n",
    "                dones[i] = True\n",
    "                agent['reached_goal'] = True\n",
    "        \n",
    "        if ((dones[0] and dones[1]) or \n",
    "            all(agent['steps_taken'] >= self.max_steps for agent in self.agents)):\n",
    "            dones = [True, True]\n",
    "            \n",
    "        self.apply_path_constraints() \n",
    "        \n",
    "        return self._get_state(), rewards, dones, {}\n",
    "    def is_valid_move(agent_id, position):\n",
    "        if position in self.obs:\n",
    "            return False\n",
    "        \n",
    "        for j, path in enumerate(self.path_history):\n",
    "            if j != agent_id and position in path:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def check_deadlock(self, agent_idx, position):\n",
    "        if position == self.agents[agent_idx]['position']:\n",
    "            blocked_directions = 0\n",
    "            for dx, dy in self.motions:\n",
    "                check_pos = (position[0] + dx, position[1] + dy)\n",
    "                if not self.validate_move(check_pos, self.agents[agent_idx]['steps_taken'] + 1, agent_idx):\n",
    "                    blocked_directions += 1\n",
    "            return blocked_directions >= 6 \n",
    "        return False\n",
    "\n",
    "    def check_path_violation(self, agent_idx, position, current_agent_idx):\n",
    "        other_agent_path = self.agent_paths[1-agent_idx][-self.path_memory_length:]\n",
    "        if not other_agent_path:\n",
    "            return False\n",
    "            \n",
    "        pos_array = np.array(position)\n",
    "        for i, path_seg_start in enumerate(other_agent_path[:-1]):\n",
    "            path_seg_end = other_agent_path[i + 1]\n",
    "            \n",
    "            corridor_width = 5.0  \n",
    "            \n",
    "            path_vector = np.array(path_seg_end) - np.array(path_seg_start)\n",
    "            path_length = np.linalg.norm(path_vector)\n",
    "            if path_length == 0:\n",
    "                continue\n",
    "                \n",
    "            perp_vector = np.array([-path_vector[1], path_vector[0]]) / path_length\n",
    "            \n",
    "            pos_relative = pos_array - np.array(path_seg_start)\n",
    "            dist_from_path = abs(np.dot(pos_relative, perp_vector))\n",
    "            \n",
    "            if dist_from_path < corridor_width:\n",
    "                proj_length = np.dot(pos_relative, path_vector) / path_length\n",
    "                if 0 <= proj_length <= path_length:\n",
    "                    return True\n",
    "        \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i != agent_idx and position == agent['goal'] and position == agent['start']:\n",
    "                return True\n",
    "                \n",
    "        loc = Location(position[0], position[1])\n",
    "        vertex_const = VertexConstraint(self.agents[current_agent_idx]['steps_taken'] + 1, loc)\n",
    "        return vertex_const in self.cbs_constraints.vertex_constraints\n",
    "\n",
    "    def is_alternative_path(self, agent_idx, position):\n",
    "        other_agent = self.agents[1-agent_idx]\n",
    "        other_pos = np.array(other_agent['position'])\n",
    "        other_goal = np.array(other_agent['goal'])\n",
    "        \n",
    "        direct_path_vector = other_goal - other_pos\n",
    "        pos_vector = np.array(position) - other_pos\n",
    "        \n",
    "        dot_product = np.dot(direct_path_vector, pos_vector)\n",
    "        norms_product = np.linalg.norm(direct_path_vector) * np.linalg.norm(pos_vector)\n",
    "        \n",
    "        if norms_product == 0:\n",
    "            return False\n",
    "            \n",
    "        angle = np.arccos(np.clip(dot_product / norms_product, -1.0, 1.0))\n",
    "        \n",
    "        return abs(angle) > np.pi/2.5\n",
    "\n",
    "    def update_path_history(self, agent_idx, position):\n",
    "        if len(self.agent_paths[agent_idx]) > self.path_memory_length:\n",
    "            keep_prob = np.linspace(0.5, 1.0, len(self.agent_paths[agent_idx]))\n",
    "            self.agent_paths[agent_idx] = [pos for pos, prob in zip(self.agent_paths[agent_idx], keep_prob)\n",
    "                                         if np.random.random() < prob]\n",
    "        self.agent_paths[agent_idx].append(position)\n",
    "\n",
    "    def compute_rewards(self, agent_idx, new_position, old_position):\n",
    "        reward = 0\n",
    "        current_dist = np.linalg.norm(np.array(new_position) - np.array(self.agents[agent_idx]['goal']))\n",
    "        progress = self.agents[agent_idx]['prev_dist'] - current_dist\n",
    "        self.agents[agent_idx]['prev_dist'] = current_dist\n",
    "        \n",
    "        other_agent_path = self.agent_paths[1-agent_idx]\n",
    "        if other_agent_path:\n",
    "            min_path_dist = float('inf')\n",
    "            pos_array = np.array(new_position)\n",
    "            \n",
    "            for path_pos in other_agent_path:\n",
    "                path_pos_array = np.array(path_pos)\n",
    "                dist = np.linalg.norm(pos_array - path_pos_array)\n",
    "                min_path_dist = min(min_path_dist, dist)\n",
    "                \n",
    "            if min_path_dist < 5.0:  \n",
    "                path_penalty = -200.0 * (5.0 - min_path_dist) \n",
    "                reward += path_penalty\n",
    "        \n",
    "        if self.is_alternative_path(agent_idx, new_position):\n",
    "            progress *= 2.0 \n",
    "            \n",
    "        reward += progress * self.progress_reward_scale\n",
    "        \n",
    "        if new_position not in self.visited_positions[agent_idx]:\n",
    "            self.visited_positions[agent_idx].add(new_position)\n",
    "            reward += 20.0  \n",
    "        if new_position == old_position and not self.is_blocked(agent_idx):\n",
    "            reward += self.stagnation_penalty * 2  \n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def is_blocked(self, agent_idx):\n",
    "        agent = self.agents[agent_idx]\n",
    "        for dx, dy in self.motions:\n",
    "            new_pos = (agent['position'][0] + dx, agent['position'][1] + dy)\n",
    "            if self.validate_move(new_pos, agent['steps_taken'] + 1, agent_idx):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_agent_path(self, agent_idx):\n",
    "        return self.prev_positions[agent_idx]\n",
    "\n",
    "    def apply_cbs_constraints(self, conflict):\n",
    "        if any(agent.get('reached_goal', False) for agent in self.agents):\n",
    "            return\n",
    "\n",
    "        if conflict.type == Conflict.VERTEX:\n",
    "            loc = conflict.location_1 \n",
    "            vertex_const = VertexConstraint(conflict.time, loc)\n",
    "            new_constraints = CBSConstraints()\n",
    "            new_constraints.vertex_constraints.add(vertex_const)\n",
    "            self.cbs_constraints.add_constraint(new_constraints)\n",
    "        elif conflict.type == Conflict.EDGE:\n",
    "            new_constraints = CBSConstraints()\n",
    "            edge_const1 = EdgeConstraint(conflict.time, conflict.location_1, conflict.location_2)\n",
    "            edge_const2 = EdgeConstraint(conflict.time, conflict.location_2, conflict.location_1)\n",
    "            new_constraints.edge_constraints.add(edge_const1)\n",
    "            new_constraints.edge_constraints.add(edge_const2)\n",
    "            self.cbs_constraints.add_constraint(new_constraints)\n",
    "            \n",
    "    def _segments_intersect(self, p1, p2, p3, p4):\n",
    "        def ccw(A, B, C):\n",
    "            return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n",
    "            \n",
    "        buffer = 0.1\n",
    "        p1 = p1 + buffer\n",
    "        p2 = p2 + buffer\n",
    "        return ccw(p1,p3,p4) != ccw(p2,p3,p4) and ccw(p1,p2,p3) != ccw(p1,p2,p4)\n",
    "\n",
    "class MultiAgentPPO:\n",
    "    def __init__(self, state_dim, action_dim, num_agents=2):\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [PPOAgent(state_dim, action_dim) for _ in range(num_agents)]\n",
    "    \n",
    "    def select_actions(self, states, training=True):\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            action, log_prob = agent.select_action(states[i], training)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "        return actions, log_probs\n",
    "    \n",
    "    def update(self, agent_trajectories):\n",
    "        for i in range(self.num_agents):\n",
    "            traj = agent_trajectories[i]\n",
    "            self.agents[i].update(\n",
    "                traj['states'],\n",
    "                traj['actions'],\n",
    "                traj['log_probs'],\n",
    "                traj['rewards'],\n",
    "                traj['next_states'],\n",
    "                traj['dones']\n",
    "            )\n",
    "\n",
    "def train_multi_agent_ppo(num_episodes=3000, max_steps=500):\n",
    "    env = MultiAgentEnv()\n",
    "    agent_wrapper = MultiAgentPPO(env.state_dim, env.action_dim)\n",
    "    \n",
    "    best_episode_paths = None\n",
    "    best_combined_reward = float('-inf')\n",
    "    best_individual_rewards = [0, 0]  \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_rewards = [0, 0]\n",
    "        paths = [[], []]\n",
    "        agent_trajectories = [\n",
    "            {'states': [], 'actions': [], 'log_probs': [], 'rewards': [], 'next_states': [], 'dones': []}\n",
    "            for _ in range(2)\n",
    "        ]\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions, log_probs = agent_wrapper.select_actions(states)\n",
    "            next_states, rewards, dones, _ = env.step(actions)\n",
    "            \n",
    "            for i in range(2):\n",
    "                agent_trajectories[i]['states'].append(states[i])\n",
    "                agent_trajectories[i]['actions'].append(actions[i])\n",
    "                agent_trajectories[i]['log_probs'].append(log_probs[i])\n",
    "                agent_trajectories[i]['rewards'].append(rewards[i])\n",
    "                agent_trajectories[i]['next_states'].append(next_states[i])\n",
    "                agent_trajectories[i]['dones'].append(dones[i])\n",
    "                paths[i].append(env.agents[i]['position'])\n",
    "                total_rewards[i] += rewards[i]\n",
    "            \n",
    "            conflict = detect_conflict(paths)\n",
    "            if conflict:\n",
    "                env.apply_cbs_constraints(conflict)\n",
    "            \n",
    "            states = next_states\n",
    "            if all(dones):\n",
    "                break\n",
    "        \n",
    "        combined_reward = sum(total_rewards)\n",
    "        if (all(env.agents[i].get('reached_goal', False) for i in range(2)) and \n",
    "            combined_reward > best_combined_reward and\n",
    "            not detect_conflict(paths)):\n",
    "            best_combined_reward = combined_reward\n",
    "            best_episode_paths = deepcopy(paths)\n",
    "            best_individual_rewards = deepcopy(total_rewards)  \n",
    "        \n",
    "        agent_wrapper.update(agent_trajectories)\n",
    "        \n",
    "        if episode % 100 == 0 or episode == num_episodes-1:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(f\"Rewards: Agent1 = {total_rewards[0]:.2f}, Agent2 = {total_rewards[1]:.2f}\")\n",
    "            visualize_multi_agent_paths(env, paths, total_rewards)\n",
    "    \n",
    "    print(\"\\nBest successful paths found:\")\n",
    "    if best_episode_paths:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        obs_x = [x for x, y in env.obs]\n",
    "        obs_y = [y for x, y in env.obs]\n",
    "        plt.plot(obs_x, obs_y, \"sk\", label=\"Obstacles\")\n",
    "    \n",
    "        for i, agent_data in enumerate(env.agents):\n",
    "            plt.plot(agent_data['start'][0], agent_data['start'][1], f\"{['b','g'][i]}s\", label=f\"Start {i+1}\")\n",
    "            plt.plot(agent_data['goal'][0], agent_data['goal'][1], f\"{['r','m'][i]}s\", label=f\"Goal {i+1}\")\n",
    "    \n",
    "        colors = ['b-', 'g-']\n",
    "        for i, path in enumerate(best_episode_paths):\n",
    "            path_x = [p[0] for p in path]\n",
    "            path_y = [p[1] for p in path]\n",
    "            plt.plot(path_x, path_y, colors[i], label=f\"Agent {i+1} Path\")\n",
    "    \n",
    "        plt.title(f\"Best Successful Paths\\nAgent 1 Reward: {best_individual_rewards[0]:.2f}, Agent 2 Reward: {best_individual_rewards[1]:.2f}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No completely successful episodes found where both agents reached their goals.\")\n",
    "    \n",
    "    return best_episode_paths, total_rewards\n",
    "\n",
    "def visualize_multi_agent_paths(env, paths, rewards):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    obs_x = [x for x, y in env.obs]\n",
    "    obs_y = [y for x, y in env.obs]\n",
    "    plt.plot(obs_x, obs_y, \"sk\", label=\"Obstacles\")\n",
    "    \n",
    "    for i, agent_data in enumerate(env.agents):\n",
    "        plt.plot(agent_data['start'][0], agent_data['start'][1], f\"{['b','g'][i]}s\", label=f\"Start {i+1}\")\n",
    "        plt.plot(agent_data['goal'][0], agent_data['goal'][1], f\"{['r','m'][i]}s\", label=f\"Goal {i+1}\")\n",
    "    \n",
    "    colors = ['b-', 'g-']\n",
    "    for i, path in enumerate(paths):\n",
    "        if len(path) == 0 or path[0] != env.agents[i]['start']:\n",
    "            full_path = [env.agents[i]['start']] + path\n",
    "        else:\n",
    "            full_path = path\n",
    "            \n",
    "        path_x = [p[0] for p in full_path]\n",
    "        path_y = [p[1] for p in full_path]\n",
    "        plt.plot(path_x, path_y, colors[i], label=f\"Agent {i+1} Path\")\n",
    "    \n",
    "    plt.title(f\"Multi-Agent Paths\\nAgent 1 Reward: {rewards[0]:.2f}, Agent 2 Reward: {rewards[1]:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_multi_agent_ppo(num_episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1b3d5",
   "metadata": {},
   "source": [
    "# MULTI-AGENT IN DYNAMIC ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5df7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CBSConstraints(object):\n",
    "    def __init__(self):\n",
    "        self.vertex_constraints = set()\n",
    "        self.edge_constraints = set()\n",
    "\n",
    "    def add_constraint(self, other):\n",
    "        self.vertex_constraints |= other.vertex_constraints\n",
    "        self.edge_constraints |= other.edge_constraints\n",
    "\n",
    "class Location(object):\n",
    "    def __init__(self, x=-1, y=-1):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    def __str__(self):\n",
    "        return str((self.x, self.y))\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, time, location):\n",
    "        self.time = time\n",
    "        self.location = location\n",
    "    def __eq__(self, other):\n",
    "        return self.time == other.time and self.location == other.location\n",
    "    def __hash__(self):\n",
    "        return hash((self.time, self.location.x, self.location.y))\n",
    "    def is_equal_except_time(self, state):\n",
    "        return self.location == state.location\n",
    "    def __str__(self):\n",
    "        return f\"({self.time}, {self.location.x}, {self.location.y})\"\n",
    "\n",
    "class Conflict(object):\n",
    "    VERTEX = 1\n",
    "    EDGE = 2\n",
    "    def __init__(self):\n",
    "        self.time = -1\n",
    "        self.type = -1\n",
    "        self.agent_1 = ''\n",
    "        self.agent_2 = ''\n",
    "        self.location_1 = Location()\n",
    "        self.location_2 = Location()\n",
    "    def __str__(self):\n",
    "        return f\"({self.time}, {self.agent_1}, {self.agent_2}, {self.location_1}, {self.location_2})\"\n",
    "\n",
    "def get_state(solution, agent_name, t):\n",
    "    if t < len(solution[agent_name]):\n",
    "        return solution[agent_name][t]\n",
    "    else:\n",
    "        return solution[agent_name][-1]\n",
    "\n",
    "def detect_first_conflict(solution):\n",
    "    max_t = max(len(path) for path in solution.values())\n",
    "    for t in range(max_t):\n",
    "        for agent1, agent2 in combinations(solution.keys(), 2):\n",
    "            state1 = get_state(solution, agent1, t)\n",
    "            state2 = get_state(solution, agent2, t)\n",
    "            if state1.is_equal_except_time(state2):\n",
    "                conflict = Conflict()\n",
    "                conflict.time = t\n",
    "                conflict.type = Conflict.VERTEX\n",
    "                conflict.agent_1 = agent1\n",
    "                conflict.agent_2 = agent2\n",
    "                conflict.location_1 = state1.location\n",
    "                return conflict\n",
    "        for agent1, agent2 in combinations(solution.keys(), 2):\n",
    "            state1a = get_state(solution, agent1, t)\n",
    "            state1b = get_state(solution, agent1, t+1)\n",
    "            state2a = get_state(solution, agent2, t)\n",
    "            state2b = get_state(solution, agent2, t+1)\n",
    "            if state1a.is_equal_except_time(state2b) and state1b.is_equal_except_time(state2a):\n",
    "                conflict = Conflict()\n",
    "                conflict.time = t\n",
    "                conflict.type = Conflict.EDGE\n",
    "                conflict.agent_1 = agent1\n",
    "                conflict.agent_2 = agent2\n",
    "                conflict.location_1 = state1a.location\n",
    "                conflict.location_2 = state1b.location\n",
    "                return conflict\n",
    "    return None\n",
    "\n",
    "def detect_conflict(paths):\n",
    "    solution = {}\n",
    "    for i, path in enumerate(paths):\n",
    "        agent_name = f\"agent{i}\"\n",
    "        state_list = []\n",
    "        for t, pos in enumerate(path):\n",
    "            state_list.append(State(t, Location(pos[0], pos[1])))\n",
    "        solution[agent_name] = state_list\n",
    "    return detect_first_conflict(solution)\n",
    "\n",
    "class VertexConstraint(object):\n",
    "    def __init__(self, time, location):\n",
    "        self.time = time\n",
    "        self.location = location\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.time == other.time and self.location == other.location\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.time) + str(self.location))\n",
    "\n",
    "class EdgeConstraint(object):\n",
    "    def __init__(self, time, location_1, location_2):\n",
    "        self.time = time\n",
    "        self.location_1 = location_1\n",
    "        self.location_2 = location_2\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.time == other.time and\n",
    "                self.location_1 == other.location_1 and\n",
    "                self.location_2 == other.location_2)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.time) + str(self.location_1) + str(self.location_2))\n",
    "\n",
    "class CBSNode:\n",
    "    def __init__(self):\n",
    "        self.solution = {}\n",
    "        self.constraints = {}\n",
    "        self.cost = 0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.cost < other.cost\n",
    "\n",
    "def compute_potential_field(pos, other_pos, other_path, goal_pos, obs, x_range, y_range):\n",
    "    pos = np.array(pos)\n",
    "    other_pos = np.array(other_pos)\n",
    "    goal_pos = np.array(goal_pos)\n",
    "\n",
    "    k_attr = 1.0\n",
    "    k_rep = 150.0\n",
    "    k_obs = 100.0\n",
    "    k_path = 100.0\n",
    "    d0 = 7.0\n",
    "    d_path = 5.0\n",
    "\n",
    "    f_attr = k_attr * (goal_pos - pos)\n",
    "\n",
    "    dist_to_other = np.linalg.norm(pos - other_pos)\n",
    "    if dist_to_other < d0:\n",
    "        f_rep = k_rep * (1/dist_to_other - 1/d0) * (1/dist_to_other**2) * (pos - other_pos)\n",
    "    else:\n",
    "        f_rep = np.zeros(2)\n",
    "\n",
    "    f_path = np.zeros(2)\n",
    "    if other_path:\n",
    "        for path_pos in other_path:\n",
    "            path_pos = np.array(path_pos)\n",
    "            dist_to_path = np.linalg.norm(pos - path_pos)\n",
    "            if dist_to_path < d_path:\n",
    "                f_path += k_path * (1/dist_to_path - 1/d_path) * (1/dist_to_path**2) * (pos - path_pos)\n",
    "\n",
    "    f_obs = np.zeros(2)\n",
    "    for dx, dy in [(0,1), (1,0), (0,-1), (-1,0), (1,1), (-1,1), (1,-1), (-1,-1)]:\n",
    "        check_pos = (int(pos[0] + dx), int(pos[1] + dy))\n",
    "        if (check_pos in obs or\n",
    "            not (0 <= check_pos[0] < x_range and 0 <= check_pos[1] < y_range) or\n",
    "            check_pos in other_path):\n",
    "            dist = np.linalg.norm(np.array([dx, dy]))\n",
    "            if dist < d0:\n",
    "                f_obs += k_obs * (1/dist - 1/d0) * (1/dist**2) * np.array([-dx, -dy])\n",
    "\n",
    "    f_total = f_attr + 2*f_rep + f_obs + 1.5*f_path\n",
    "\n",
    "    f_norm = np.linalg.norm(f_total)\n",
    "    if f_norm > 0:\n",
    "        f_total = f_total / f_norm\n",
    "\n",
    "    return f_total\n",
    "\n",
    "class MultiAgentEnv:\n",
    "    def __init__(self):\n",
    "        self.x_range = 61\n",
    "        self.y_range = 41\n",
    "        self.motions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                        (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.obs = self.obs_map()\n",
    "        self.action_dim = len(self.motions)\n",
    "        self.max_steps = 500\n",
    "        self.cbs_constraints = CBSConstraints()\n",
    "        self.agent_paths = [[], []]\n",
    "\n",
    "        self.safety_radius = 3\n",
    "        self.collision_penalty = -40\n",
    "        self.path_penalty = -20\n",
    "        self.min_path_distance = 2\n",
    "        self.path_memory_length = 5\n",
    "\n",
    "        self.stagnation_penalty = -400\n",
    "\n",
    "        self.goal_reached_bonus = 2000.0\n",
    "        self.progress_reward_scale = 30.0\n",
    "\n",
    "        self.agents = [\n",
    "            {'start': (10, 30), 'goal': (55, 5), 'position': (10, 30), 'steps_taken': 0},\n",
    "            {'start': (5, 5), 'goal': (45, 35), 'position': (5, 5), 'steps_taken': 0}\n",
    "        ]\n",
    "\n",
    "        self.path_crossing_cooldown = 20\n",
    "        self.last_crossing = [0, 0]\n",
    "        self.alternative_path_bonus = 15\n",
    "        self.deadlock_threshold = 15\n",
    "        self.deadlock_counter = [0, 0]\n",
    "        self.path_history = [[] for _ in range(len(self.agents))]\n",
    "        self.path_obstacle_memory = 5\n",
    "        self.moving_obstacles = [\n",
    "            {\n",
    "                'path_type': 'circle',\n",
    "                'radius': 8,\n",
    "                'speed': 0.1,\n",
    "                'center': (15, 25),\n",
    "                'position': [15, 25],\n",
    "                'angle': 0\n",
    "            },\n",
    "            {\n",
    "                'path_type': 'vertical',\n",
    "                'y_min': 10,\n",
    "                'y_max': 35,\n",
    "                'speed': 0.2,\n",
    "                'position': [35, 25],\n",
    "                'direction': 1\n",
    "            },\n",
    "            {\n",
    "                'path_type': 'horizontal',\n",
    "                'x_min': 15,\n",
    "                'x_max': 35,\n",
    "                'speed': 0.15,\n",
    "                'position': [35, 15],\n",
    "                'direction': 1\n",
    "            },\n",
    "            {\n",
    "                'path_type': 'figure_eight',\n",
    "                'a': 5,\n",
    "                'b': 3,\n",
    "                'center': (45, 10),\n",
    "                'position': [45, 10],\n",
    "                'speed': 0.05,\n",
    "                'angle': 0\n",
    "            },\n",
    "            {\n",
    "                'path_type': 'diagonal',\n",
    "                'x_min': 10,\n",
    "                'x_max': 25,\n",
    "                'y_min': 10,\n",
    "                'y_max': 20,\n",
    "                'speed': 0.1,\n",
    "                'position': [15, 5],\n",
    "                'direction_x': 1,\n",
    "                'direction_y': 1\n",
    "            }\n",
    "        ]\n",
    "        self.state_dim = 23 + (4 * len(self.moving_obstacles))\n",
    "\n",
    "    def apply_path_constraints(self):\n",
    "        for t in range(len(self.agent_paths[0])):\n",
    "            for i in range(len(self.agent_paths)):\n",
    "                if t < len(self.agent_paths[i]):\n",
    "                    loc = Location(self.agent_paths[i][t][0], self.agent_paths[i][t][1])\n",
    "                    vertex_const = VertexConstraint(t, loc)\n",
    "                    self.cbs_constraints.vertex_constraints.add(vertex_const)\n",
    "\n",
    "    def obs_map(self):\n",
    "        x = self.x_range\n",
    "        y = self.y_range\n",
    "        obs = set()\n",
    "        for i in range(x):\n",
    "            obs.add((i, 0))\n",
    "        for i in range(x):\n",
    "            obs.add((i, y - 1))\n",
    "        for i in range(y):\n",
    "            obs.add((0, i))\n",
    "        for i in range(y):\n",
    "            obs.add((x - 1, i))\n",
    "        for i in range(10, 21):\n",
    "            obs.add((i, 15))\n",
    "        for i in range(15):\n",
    "            obs.add((20, i))\n",
    "        for i in range(15, 30):\n",
    "            obs.add((30, i))\n",
    "        for i in range(16):\n",
    "            obs.add((40, i))\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_paths = [[], []]\n",
    "\n",
    "        self.agents = [\n",
    "            {'start': (10, 30), 'goal': (55, 5), 'position': (10, 30), 'steps_taken': 0},\n",
    "            {'start': (5, 5), 'goal': (45, 35), 'position': (5, 5), 'steps_taken': 0}\n",
    "        ]\n",
    "        self.visited_positions = [{agent['start']} for agent in self.agents]\n",
    "        self.exploration_bonuses = [{} for _ in range(2)]\n",
    "        self.stuck_counters = [0, 0]\n",
    "        self.prev_positions = [[], []]\n",
    "        for agent in self.agents:\n",
    "            start = np.array(agent['start'])\n",
    "            goal = np.array(agent['goal'])\n",
    "            agent['prev_dist'] = np.linalg.norm(start - goal)\n",
    "        self.obs = self.obs_map()\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        states = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            pos = np.array(agent['position'])\n",
    "            goal = np.array(agent['goal'])\n",
    "            other_agent_pos = np.array(self.agents[1-i]['position'])\n",
    "            other_agent_path = self.agent_paths[1-i]\n",
    "\n",
    "            min_path_dist = float('inf')\n",
    "            if other_agent_path:\n",
    "                for path_pos in other_agent_path:\n",
    "                    dist = np.linalg.norm(np.array(path_pos) - pos)\n",
    "                    min_path_dist = min(min_path_dist, dist)\n",
    "            min_path_dist = min(min_path_dist, np.sqrt(self.x_range**2 + self.y_range**2))\n",
    "\n",
    "            dist_to_goal = np.linalg.norm(pos - goal)\n",
    "            dist_to_other_agent = np.linalg.norm(pos - other_agent_pos)\n",
    "            angle_to_goal = np.arctan2(goal[1]-pos[1], goal[0]-pos[0]) / np.pi\n",
    "            angle_to_other_agent = np.arctan2(other_agent_pos[1]-pos[1], other_agent_pos[0]-pos[0]) / np.pi\n",
    "\n",
    "            other_agent_prev_pos = np.array(self.agents[1-i].get('prev_position', other_agent_pos))\n",
    "            relative_velocity = (other_agent_pos - other_agent_prev_pos)\n",
    "            rel_vel_x = relative_velocity[0] / self.x_range\n",
    "            rel_vel_y = relative_velocity[1] / self.y_range\n",
    "\n",
    "            if np.any(relative_velocity):\n",
    "                time_to_collision = dist_to_other_agent / np.linalg.norm(relative_velocity)\n",
    "                time_to_collision = np.clip(time_to_collision / 10.0, 0, 1)\n",
    "            else:\n",
    "                time_to_collision = 1.0\n",
    "\n",
    "            obstacle_dists = []\n",
    "            for dx, dy in self.motions:\n",
    "                x, y = agent['position']\n",
    "                dist = 0\n",
    "                while True:\n",
    "                    x += dx\n",
    "                    y += dy\n",
    "                    dist += 1\n",
    "                    next_pos = (x, y)\n",
    "                    if (next_pos in self.obs or\n",
    "                        not (0 <= x < self.x_range and 0 <= y < self.y_range) or\n",
    "                        next_pos in other_agent_path or\n",
    "                        (np.linalg.norm(np.array([x, y]) - other_agent_pos) < self.safety_radius)):\n",
    "                        obstacle_dists.append(min(dist/10.0, 1.0))\n",
    "                        break\n",
    "\n",
    "            norm_path_dist = min_path_dist / np.sqrt(self.x_range**2 + self.y_range**2)\n",
    "\n",
    "            state = np.array([\n",
    "                pos[0] / self.x_range,\n",
    "                pos[1] / self.y_range,\n",
    "                goal[0] / self.x_range,\n",
    "                goal[1] / self.y_range,\n",
    "                dist_to_goal / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "                angle_to_goal,\n",
    "                other_agent_pos[0] / self.x_range,\n",
    "                other_agent_pos[1] / self.y_range,\n",
    "                dist_to_other_agent / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "                angle_to_other_agent,\n",
    "                rel_vel_x,\n",
    "                rel_vel_y,\n",
    "                time_to_collision,\n",
    "                norm_path_dist,\n",
    "                *obstacle_dists,\n",
    "                agent['steps_taken'] / self.max_steps\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            obstacle_info = []\n",
    "            for obstacle in self.moving_obstacles:\n",
    "                obstacle_pos = np.array(obstacle['position'])\n",
    "                dist = np.linalg.norm(pos - obstacle_pos)\n",
    "                angle = np.arctan2(obstacle_pos[1]-pos[1], obstacle_pos[0]-pos[0]) / np.pi\n",
    "\n",
    "                if obstacle['path_type'] == 'circle':\n",
    "                    dx = -obstacle['radius'] * np.sin(obstacle['angle']) * obstacle['speed']\n",
    "                    dy = obstacle['radius'] * np.cos(obstacle['angle']) * obstacle['speed']\n",
    "                elif obstacle['path_type'] == 'vertical':\n",
    "                    dx = 0\n",
    "                    dy = obstacle['speed'] * obstacle['direction']\n",
    "                elif obstacle['path_type'] == 'horizontal':\n",
    "                    dx = obstacle['speed'] * obstacle['direction']\n",
    "                    dy = 0\n",
    "                elif obstacle['path_type'] == 'figure_eight':\n",
    "                    dx = obstacle['a'] * np.cos(obstacle['angle']) * obstacle['speed']\n",
    "                    dy = obstacle['b'] * np.cos(obstacle['angle'] * 2) * 2 * obstacle['speed']\n",
    "                elif obstacle['path_type'] == 'diagonal':\n",
    "                    dx = obstacle['speed'] * obstacle['direction_x']\n",
    "                    dy = obstacle['speed'] * obstacle['direction_y']\n",
    "\n",
    "                obstacle_info.extend([\n",
    "                    dist / np.sqrt(self.x_range**2 + self.y_range**2),\n",
    "                    angle,\n",
    "                    dx / self.x_range,\n",
    "                    dy / self.y_range\n",
    "                ])\n",
    "\n",
    "            state = np.append(state, obstacle_info)\n",
    "            states.append(state)\n",
    "        return states\n",
    "\n",
    "    def get_exploration_bonus(self, agent_idx, position):\n",
    "        if position not in self.exploration_bonuses[agent_idx]:\n",
    "            self.exploration_bonuses[agent_idx][position] = 1.0\n",
    "        else:\n",
    "            self.exploration_bonuses[agent_idx][position] *= self.exploration_decay\n",
    "            self.exploration_bonuses[agent_idx][position] = max(\n",
    "                self.exploration_bonuses[agent_idx][position],\n",
    "                self.min_exploration_bonus\n",
    "            )\n",
    "        return self.exploration_bonuses[agent_idx][position]\n",
    "\n",
    "    def validate_move(self, pos, time, agent_id):\n",
    "        x, y = pos\n",
    "        if not (0 <= x < self.x_range and 0 <= y < self.y_range):\n",
    "            return False\n",
    "        if pos in self.obs:\n",
    "            return False\n",
    "\n",
    "        other_agent_path = self.agent_paths[1-agent_id]\n",
    "        other_agent_pos = self.agents[1-agent_id]['position']\n",
    "        pos_array = np.array(pos)\n",
    "        safety_distance = 4.0\n",
    "\n",
    "        other_pos_array = np.array(other_agent_pos)\n",
    "        if np.linalg.norm(pos_array - other_pos_array) < safety_distance:\n",
    "            return False\n",
    "\n",
    "        if other_agent_path:\n",
    "            for path_pos in other_agent_path:\n",
    "                path_pos_array = np.array(path_pos)\n",
    "                dist = np.linalg.norm(pos_array - path_pos_array)\n",
    "                if dist < safety_distance:\n",
    "                    return False\n",
    "\n",
    "        if len(other_agent_path) >= 2:\n",
    "            current_pos = np.array(self.agents[agent_id]['position'])\n",
    "\n",
    "            for i in range(len(other_agent_path) - 1):\n",
    "                path_seg_start = np.array(other_agent_path[i])\n",
    "                path_seg_end = np.array(other_agent_path[i + 1])\n",
    "                corridor_width = 5.0\n",
    "                path_vector = path_seg_end - path_seg_start\n",
    "                path_length = np.linalg.norm(path_vector)\n",
    "                if path_length == 0:\n",
    "                    continue\n",
    "\n",
    "                perp_vector = np.array([-path_vector[1], path_vector[0]]) / path_length\n",
    "\n",
    "                pos_relative = pos_array - path_seg_start\n",
    "                dist_from_path = abs(np.dot(pos_relative, perp_vector))\n",
    "\n",
    "                if dist_from_path < corridor_width:\n",
    "                    proj_length = np.dot(pos_relative, path_vector) / path_length\n",
    "                    if 0 <= proj_length <= path_length:\n",
    "                        return False\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i != agent_id and pos == agent['goal'] and pos == agent['start']:\n",
    "                return False\n",
    "\n",
    "        loc = Location(pos[0], pos[1])\n",
    "        vertex_const = VertexConstraint(time, loc)\n",
    "        if vertex_const in self.cbs_constraints.vertex_constraints:\n",
    "            return False\n",
    "\n",
    "        for obstacle in self.moving_obstacles:\n",
    "            obstacle_pos = np.array(obstacle['position'])\n",
    "            safety_distance = 2.0\n",
    "            if np.linalg.norm(pos_array - obstacle_pos) < safety_distance:\n",
    "                return False\n",
    "\n",
    "        return True  \n",
    "\n",
    "    def validate_transition(self, pos1, pos2,time):\n",
    "        loc1 = Location(pos1[0], pos1[1])\n",
    "        loc2 = Location(pos2[0], pos2[1])\n",
    "        edge_const = EdgeConstraint(time, loc1, loc2)\n",
    "        return edge_const not in self.cbs_constraints.edge_constraints\n",
    "\n",
    "    def check_stuck(self, agent_idx, position):\n",
    "        self.prev_positions[agent_idx].append(position)\n",
    "        if len(self.prev_positions[agent_idx]) > 10:\n",
    "            self.prev_positions[agent_idx].pop(0)\n",
    "        if len(self.prev_positions[agent_idx]) == 10:\n",
    "            unique_positions = len(set(self.prev_positions[agent_idx]))\n",
    "            if unique_positions <= 3:\n",
    "                self.stuck_counters[agent_idx] += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.update_moving_obstacles()\n",
    "\n",
    "        rewards = [0, 0]\n",
    "        dones = [False, False]\n",
    "        prev_positions = [agent['position'] for agent in self.agents]\n",
    "        prev_distances = [agent['prev_dist'] for agent in self.agents]\n",
    "        current_time = self.agents[0]['steps_taken']\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            position = agent['position']\n",
    "            self.path_history[i].append(position)\n",
    "\n",
    "            if len(self.path_history[i]) > self.path_obstacle_memory:\n",
    "                self.path_history[i].pop(0)\n",
    "        candidate_positions = []\n",
    "        for i, (agent, action) in enumerate(zip(self.agents, actions)):\n",
    "            if agent.get('reached_goal', False):\n",
    "                candidate_positions.append(agent['position'])\n",
    "                continue\n",
    "\n",
    "            x, y = agent['position']\n",
    "            dx, dy = self.motions[action]\n",
    "            new_pos = (x+dx, y+dy)\n",
    "\n",
    "            if self.check_deadlock(i, new_pos):\n",
    "                self.deadlock_counter[i] += 1\n",
    "            else:\n",
    "                self.deadlock_counter[i] = 0\n",
    "\n",
    "            path_violation = self.check_path_violation(i, new_pos,i)\n",
    "            other_agent_dist = np.linalg.norm(np.array(new_pos) - np.array(self.agents[1-i]['position']))\n",
    "\n",
    "            if path_violation:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.path_penalty * 0.5\n",
    "            elif new_pos == self.agents[1-i]['position']:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.collision_penalty\n",
    "            elif other_agent_dist < self.safety_radius:\n",
    "                current_dist_to_goal = np.linalg.norm(np.array(new_pos) - np.array(agent['goal']))\n",
    "                if current_dist_to_goal < agent['prev_dist']:\n",
    "                    candidate_positions.append(new_pos)\n",
    "                    rewards[i] += self.collision_penalty * 0.2\n",
    "                else:\n",
    "                    candidate_positions.append(agent['position'])\n",
    "                    rewards[i] += self.collision_penalty * 0.3\n",
    "            elif self.validate_move(new_pos, current_time + 1, i):\n",
    "                candidate_positions.append(new_pos)\n",
    "                if self.is_alternative_path(i, new_pos):\n",
    "                    rewards[i] += self.alternative_path_bonus\n",
    "            else:\n",
    "                candidate_positions.append(agent['position'])\n",
    "                rewards[i] += self.collision_penalty * 0.2\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if agent.get('reached_goal', False):\n",
    "                continue\n",
    "\n",
    "            new_position = candidate_positions[i]\n",
    "            old_position = agent['position']\n",
    "            agent['position'] = new_position\n",
    "            agent['steps_taken'] += 1\n",
    "\n",
    "            if self.deadlock_counter[i] >= self.deadlock_threshold:\n",
    "                self.min_path_distance = max(1, self.min_path_distance - 0.5)\n",
    "                rewards[i] += self.stagnation_penalty * 5\n",
    "            else:\n",
    "                self.min_path_distance = min(2, self.min_path_distance + 0.1)\n",
    "\n",
    "            self.update_path_history(i, new_position)\n",
    "\n",
    "            rewards[i] += self.compute_rewards(i, new_position, old_position)\n",
    "\n",
    "            if new_position == agent['goal']:\n",
    "                rewards[i] += self.goal_reached_bonus\n",
    "                dones[i] = True\n",
    "                agent['reached_goal'] = True\n",
    "\n",
    "        if ((dones[0] and dones[1]) or\n",
    "            all(agent['steps_taken'] >= self.max_steps for agent in self.agents)):\n",
    "            dones = [True, True]\n",
    "\n",
    "        self.apply_path_constraints()\n",
    "\n",
    "        return self._get_state(), rewards, dones, {}\n",
    "\n",
    "    def is_valid_move(agent_id, position):\n",
    "        if position in self.obs:\n",
    "            return False\n",
    "\n",
    "        for j, path in enumerate(self.path_history):\n",
    "            if j != agent_id and position in path:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def check_deadlock(self, agent_idx, position):\n",
    "        if position == self.agents[agent_idx]['position']:\n",
    "            blocked_directions = 0\n",
    "            for dx, dy in self.motions:\n",
    "                check_pos = (position[0] + dx, position[1] + dy)\n",
    "                if not self.validate_move(check_pos, self.agents[agent_idx]['steps_taken'] + 1, agent_idx):\n",
    "                    blocked_directions += 1\n",
    "            return blocked_directions >= 6\n",
    "        return False\n",
    "\n",
    "    def check_path_violation(self, agent_idx, position, current_agent_idx):\n",
    "        other_agent_path = self.agent_paths[1-agent_idx][-self.path_memory_length:]\n",
    "        if not other_agent_path:\n",
    "            return False\n",
    "\n",
    "        pos_array = np.array(position)\n",
    "        for i, path_seg_start in enumerate(other_agent_path[:-1]):\n",
    "            path_seg_end = other_agent_path[i + 1]\n",
    "\n",
    "            corridor_width = 5.0\n",
    "\n",
    "            path_vector = np.array(path_seg_end) - np.array(path_seg_start)\n",
    "            path_length = np.linalg.norm(path_vector)\n",
    "            if path_length == 0:\n",
    "                continue\n",
    "\n",
    "            perp_vector = np.array([-path_vector[1], path_vector[0]]) / path_length\n",
    "\n",
    "            pos_relative = pos_array - np.array(path_seg_start)\n",
    "            dist_from_path = abs(np.dot(pos_relative, perp_vector))\n",
    "\n",
    "            if dist_from_path < corridor_width:\n",
    "                proj_length = np.dot(pos_relative, path_vector) / path_length\n",
    "                if 0 <= proj_length <= path_length:\n",
    "                    return True\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i != agent_idx and position == agent['goal'] and position == agent['start']:\n",
    "                return True\n",
    "\n",
    "        loc = Location(position[0], position[1])\n",
    "        vertex_const = VertexConstraint(self.agents[current_agent_idx]['steps_taken'] + 1, loc)\n",
    "        return vertex_const in self.cbs_constraints.vertex_constraints\n",
    "\n",
    "    def is_alternative_path(self, agent_idx, position):\n",
    "        other_agent = self.agents[1-agent_idx]\n",
    "        other_pos = np.array(other_agent['position'])\n",
    "        other_goal = np.array(other_agent['goal'])\n",
    "\n",
    "        direct_path_vector = other_goal - other_pos\n",
    "        pos_vector = np.array(position) - other_pos\n",
    "\n",
    "        dot_product = np.dot(direct_path_vector, pos_vector)\n",
    "        norms_product = np.linalg.norm(direct_path_vector) * np.linalg.norm(pos_vector)\n",
    "\n",
    "        if norms_product == 0:\n",
    "            return False\n",
    "\n",
    "        angle = np.arccos(np.clip(dot_product / norms_product, -1.0, 1.0))\n",
    "\n",
    "        return abs(angle) > np.pi/2.5\n",
    "\n",
    "    def update_path_history(self, agent_idx, position):\n",
    "        if len(self.agent_paths[agent_idx]) > self.path_memory_length:\n",
    "            keep_prob = np.linspace(0.5, 1.0, len(self.agent_paths[agent_idx]))\n",
    "            self.agent_paths[agent_idx] = [pos for pos, prob in zip(self.agent_paths[agent_idx], keep_prob)\n",
    "                                        if np.random.random() < prob]\n",
    "        self.agent_paths[agent_idx].append(position)\n",
    "\n",
    "    def compute_rewards(self, agent_idx, new_position, old_position):\n",
    "        reward = 0\n",
    "        current_dist = np.linalg.norm(np.array(new_position) - np.array(self.agents[agent_idx]['goal']))\n",
    "        progress = self.agents[agent_idx]['prev_dist'] - current_dist\n",
    "        self.agents[agent_idx]['prev_dist'] = current_dist\n",
    "\n",
    "        other_agent_path = self.agent_paths[1-agent_idx]\n",
    "        if other_agent_path:\n",
    "            min_path_dist = float('inf')\n",
    "            pos_array = np.array(new_position)\n",
    "\n",
    "            for path_pos in other_agent_path:\n",
    "                path_pos_array = np.array(path_pos)\n",
    "                dist = np.linalg.norm(pos_array - path_pos_array)\n",
    "                min_path_dist = min(min_path_dist, dist)\n",
    "\n",
    "            if min_path_dist < 5.0:\n",
    "                path_penalty = -200.0 * (5.0 - min_path_dist)\n",
    "                reward += path_penalty\n",
    "\n",
    "        if self.is_alternative_path(agent_idx, new_position):\n",
    "            progress *= 2.0\n",
    "\n",
    "        reward += progress * self.progress_reward_scale\n",
    "\n",
    "        if new_position not in self.visited_positions[agent_idx]:\n",
    "            self.visited_positions[agent_idx].add(new_position)\n",
    "            reward += 20.0\n",
    "        if new_position == old_position and not self.is_blocked(agent_idx):\n",
    "            reward += self.stagnation_penalty * 2\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def is_blocked(self, agent_idx):\n",
    "        agent = self.agents[agent_idx]\n",
    "        for dx, dy in self.motions:\n",
    "            new_pos = (agent['position'][0] + dx, agent['position'][1] + dy)\n",
    "            if self.validate_move(new_pos, agent['steps_taken'] + 1, agent_idx):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_agent_path(self, agent_idx):\n",
    "        return self.prev_positions[agent_idx]\n",
    "\n",
    "    def apply_cbs_constraints(self, conflict):\n",
    "        if any(agent.get('reached_goal', False) for agent in self.agents):\n",
    "            return\n",
    "\n",
    "        if conflict.type == Conflict.VERTEX:\n",
    "            loc = conflict.location_1\n",
    "            vertex_const = VertexConstraint(conflict.time, loc)\n",
    "            new_constraints = CBSConstraints()\n",
    "            new_constraints.vertex_constraints.add(vertex_const)\n",
    "            self.cbs_constraints.add_constraint(new_constraints)\n",
    "        elif conflict.type == Conflict.EDGE:\n",
    "            new_constraints = CBSConstraints()\n",
    "            edge_const1 = EdgeConstraint(conflict.time, conflict.location_1, conflict.location_2)\n",
    "            edge_const2 = EdgeConstraint(conflict.time, conflict.location_2, conflict.location_1)\n",
    "            new_constraints.edge_constraints.add(edge_const1)\n",
    "            new_constraints.edge_constraints.add(edge_const2)\n",
    "            self.cbs_constraints.add_constraint(new_constraints)\n",
    "\n",
    "    def _segments_intersect(self, p1, p2, p3, p4):\n",
    "        def ccw(A, B, C):\n",
    "            return (C[1]-A[1]) * (B[0]-A[0]) > (B[1]-A[1]) * (C[0]-A[0])\n",
    "\n",
    "        buffer = 0.1\n",
    "        p1 = p1 + buffer\n",
    "        p2 = p2 + buffer\n",
    "        return ccw(p1,p3,p4) != ccw(p2,p3,p4) and ccw(p1,p2,p3) != ccw(p1,p2,p4)\n",
    "\n",
    "    def update_moving_obstacles(self):\n",
    "        for obstacle in self.moving_obstacles:\n",
    "            if obstacle['path_type'] == 'circle':\n",
    "                obstacle['angle'] += obstacle['speed']\n",
    "                obstacle['position'][0] = obstacle['center'][0] + obstacle['radius'] * np.cos(obstacle['angle'])\n",
    "                obstacle['position'][1] = obstacle['center'][1] + obstacle['radius'] * np.sin(obstacle['angle'])\n",
    "\n",
    "            elif obstacle['path_type'] == 'vertical':\n",
    "                obstacle['position'][1] += obstacle['speed'] * obstacle['direction']\n",
    "                if obstacle['position'][1] >= obstacle['y_max']:\n",
    "                    obstacle['direction'] = -1\n",
    "                elif obstacle['position'][1] <= obstacle['y_min']:\n",
    "                    obstacle['direction'] = 1\n",
    "\n",
    "            elif obstacle['path_type'] == 'horizontal':\n",
    "                obstacle['position'][0] += obstacle['speed'] * obstacle['direction']\n",
    "                if obstacle['position'][0] >= obstacle['x_max']:\n",
    "                    obstacle['direction'] = -1\n",
    "                elif obstacle['position'][0] <= obstacle['x_min']:\n",
    "                    obstacle['direction'] = 1\n",
    "\n",
    "            elif obstacle['path_type'] == 'figure_eight':\n",
    "                obstacle['angle'] += obstacle['speed']\n",
    "                obstacle['position'][0] = obstacle['center'][0] + obstacle['a'] * np.sin(obstacle['angle'])\n",
    "                obstacle['position'][1] = obstacle['center'][1] + obstacle['b'] * np.sin(obstacle['angle'] * 2)\n",
    "\n",
    "            elif obstacle['path_type'] == 'diagonal':\n",
    "                obstacle['position'][0] += obstacle['speed'] * obstacle['direction_x']\n",
    "                obstacle['position'][1] += obstacle['speed'] * obstacle['direction_y']\n",
    "\n",
    "                if obstacle['position'][0] >= obstacle['x_max']:\n",
    "                    obstacle['direction_x'] = -1\n",
    "                elif obstacle['position'][0] <= obstacle['x_min']:\n",
    "                    obstacle['direction_x'] = 1\n",
    "\n",
    "                if obstacle['position'][1] >= obstacle['y_max']:\n",
    "                    obstacle['direction_y'] = -1\n",
    "                elif obstacle['position'][1] <= obstacle['y_min']:\n",
    "                    obstacle['direction_y'] = 1\n",
    "\n",
    "class MultiAgentPPO:\n",
    "    def __init__(self, state_dim, action_dim, num_agents=2):\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [PPOAgent(state_dim, action_dim) for _ in range(num_agents)]\n",
    "\n",
    "    def select_actions(self, states, training=True):\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            action, log_prob = agent.select_action(states[i], training)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "        return actions, log_probs\n",
    "\n",
    "    def update(self, agent_trajectories):\n",
    "        for i in range(self.num_agents):\n",
    "            traj = agent_trajectories[i]\n",
    "            self.agents[i].update(\n",
    "                traj['states'],\n",
    "                traj['actions'],\n",
    "                traj['log_probs'],\n",
    "                traj['rewards'],\n",
    "                traj['next_states'],\n",
    "                traj['dones']\n",
    "            )\n",
    "def visualize_multi_agent_paths(env, paths, rewards):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    obs_x = [x for x, y in env.obs]\n",
    "    obs_y = [y for x, y in env.obs]\n",
    "    plt.plot(obs_x, obs_y, \"sk\", label=\"Obstacles\")\n",
    "    # Plot dynamic obstacles as static obstacles\n",
    "    for obs in env.moving_obstacles:\n",
    "        plt.plot(obs['position'][0], obs['position'][1], \"sk\", label=\"Dynamic Obstacles (Treated as Static)\")\n",
    "\n",
    "    for i, agent_data in enumerate(env.agents):\n",
    "        plt.plot(agent_data['start'][0], agent_data['start'][1], f\"{['b','g'][i]}s\", label=f\"Start {i+1}\")\n",
    "        plt.plot(agent_data['goal'][0], agent_data['goal'][1], f\"{['r','m'][i]}s\", label=f\"Goal {i+1}\")\n",
    "\n",
    "    colors = ['b-', 'g-']\n",
    "    for i, path in enumerate(paths):\n",
    "        if len(path) == 0 or path[0] != env.agents[i]['start']:\n",
    "            full_path = [env.agents[i]['start']] + path\n",
    "        else:\n",
    "            full_path = path\n",
    "\n",
    "        path_x = [p[0] for p in full_path]\n",
    "        path_y = [p[1] for p in full_path]\n",
    "        plt.plot(path_x, path_y, colors[i], label=f\"Agent {i+1} Path\")\n",
    "\n",
    "    if isinstance(rewards, (list, tuple)):\n",
    "        title = f\"Multi-Agent Paths\\nAgent 1 Reward: {rewards[0]:.2f}, Agent 2 Reward: {rewards[1]:.2f}\"\n",
    "    else:\n",
    "        title = f\"Multi-Agent Paths\\nTotal Reward: {rewards:.2f}\"\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "def train_multi_agent_ppo(num_episodes=3000, max_steps=500):\n",
    "    env = MultiAgentEnv()\n",
    "    agent_wrapper = MultiAgentPPO(env.state_dim, env.action_dim)\n",
    "\n",
    "    best_episode_paths = None\n",
    "    best_combined_reward = float('-inf')\n",
    "    last_success_paths = None\n",
    "    last_success_rewards = None\n",
    "    last_success_lengths = None\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_rewards = [0, 0]\n",
    "        paths = [[], []]\n",
    "        agent_trajectories = [\n",
    "            {'states': [], 'actions': [], 'log_probs': [], 'rewards': [],\n",
    "            'next_states': [], 'dones': []}\n",
    "            for _ in range(2)\n",
    "        ]\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            actions, log_probs = agent_wrapper.select_actions(states)\n",
    "            next_states, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            for i in range(2):\n",
    "                agent_trajectories[i]['states'].append(states[i])\n",
    "                agent_trajectories[i]['actions'].append(actions[i])\n",
    "                agent_trajectories[i]['log_probs'].append(log_probs[i])\n",
    "                agent_trajectories[i]['rewards'].append(rewards[i])\n",
    "                agent_trajectories[i]['next_states'].append(next_states[i])\n",
    "                agent_trajectories[i]['dones'].append(dones[i])\n",
    "                paths[i].append(env.agents[i]['position'])\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            conflict = detect_conflict(paths)\n",
    "            if conflict:\n",
    "                env.apply_cbs_constraints(conflict)\n",
    "\n",
    "            states = next_states\n",
    "            if all(dones):\n",
    "                break\n",
    "\n",
    "        combined_reward = sum(total_rewards)\n",
    "\n",
    "        if all(env.agents[i].get('reached_goal', False) for i in range(2)):\n",
    "            last_success_paths = deepcopy(paths)\n",
    "            last_success_rewards = total_rewards.copy()\n",
    "            last_success_lengths = [len(paths[0]), len(paths[1])]\n",
    "\n",
    "            if combined_reward > best_combined_reward and not detect_conflict(paths):\n",
    "                best_combined_reward = combined_reward\n",
    "                best_episode_paths = deepcopy(paths)\n",
    "\n",
    "        agent_wrapper.update(agent_trajectories)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(f\"Rewards: Agent1 = {total_rewards[0]:.2f}, Agent2 = {total_rewards[1]:.2f}\")\n",
    "\n",
    "    print(\"\\nTraining Complete!\")\n",
    "    if best_episode_paths:\n",
    "        print(f\"Best path found - Combined Reward: {best_combined_reward:.2f}\")\n",
    "        print(\"Visualizing best path found during training:\")\n",
    "        visualize_multi_agent_paths(env, best_episode_paths, best_combined_reward)\n",
    "\n",
    "    print(\"\\nLast successful path statistics:\")\n",
    "    if last_success_paths:\n",
    "        print(f\"Rewards: Agent1 = {last_success_rewards[0]:.2f}, Agent2 = {last_success_rewards[1]:.2f}\")\n",
    "        print(f\"Lengths: Agent1 = {last_success_lengths[0]}, Agent2 = {last_success_lengths[1]}\")\n",
    "        print(\"Visualizing last successful paths:\")\n",
    "        visualize_multi_agent_paths(env, last_success_paths, sum(last_success_rewards))\n",
    "    else:\n",
    "        print(\"No successful paths found during training\")\n",
    "\n",
    "    return last_success_paths,best_episode_paths, env\n",
    "\n",
    "def calculate_path_straightness(path):\n",
    "    if len(path) < 2:\n",
    "        return float('inf')\n",
    "\n",
    "    start, end = path[0], path[-1]\n",
    "    total_deviation = 0\n",
    "    for point in path:\n",
    "        num = abs((end[1] - start[1]) * point[0] - (end[0] - start[0]) * point[1] + end[0] * start[1] - end[1] * start[0])\n",
    "        den = np.sqrt((end[1] - start[1])**2 + (end[0] - start[0])**2)\n",
    "        total_deviation += num / den\n",
    "    return total_deviation\n",
    "\n",
    "def find_straightest_path(paths):\n",
    "    straightest_path = None\n",
    "    min_deviation = float('inf')\n",
    "    for path in paths:\n",
    "        deviation = calculate_path_straightness(path)\n",
    "        if deviation < min_deviation:\n",
    "            min_deviation = deviation\n",
    "            straightest_path = path\n",
    "    return straightest_path\n",
    "\n",
    "def animate_last_successful_path(env, last_successful_path):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    obs_x = [x for x, y in env.obs]\n",
    "    obs_y = [y for x, y in env.obs]\n",
    "    ax.plot(obs_x, obs_y, \"sk\", markersize=5, label=\"Obstacles\")\n",
    "\n",
    "    moving_obstacles = []\n",
    "    safety_circles = []\n",
    "    obstacle_data = deepcopy(env.moving_obstacles)\n",
    "    \n",
    "    for obs_data in obstacle_data:\n",
    "        obstacle = ax.plot([], [], 'ro', markersize=8)[0]\n",
    "        moving_obstacles.append(obstacle)\n",
    "\n",
    "        safety = plt.Circle((0, 0), 2.0, color='red', alpha=0.1)\n",
    "        ax.add_patch(safety)\n",
    "        safety_circles.append(safety)\n",
    "\n",
    "    agents = []\n",
    "    paths = []\n",
    "    agent_safety_circles = []\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    for i, agent_data in enumerate(env.agents):\n",
    "        ax.plot(agent_data['start'][0], agent_data['start'][1],\n",
    "                f\"{colors[i % len(colors)]}s\", markersize=10, label=f\"Start {i+1}\")\n",
    "        ax.plot(agent_data['goal'][0], agent_data['goal'][1],\n",
    "                'rs', markersize=10, label=f\"Goal {i+1}\")\n",
    "\n",
    "        agent = ax.plot([], [], f\"{colors[i % len(colors)]}o\", markersize=8)[0]\n",
    "        path = ax.plot([], [], f\"{colors[i % len(colors)]}-\", alpha=0.5,\n",
    "                    label=f\"Agent {i+1} Path\")[0]\n",
    "\n",
    "        safety = plt.Circle((0, 0), env.safety_radius, color=colors[i % len(colors)], alpha=0.1)\n",
    "        ax.add_patch(safety)\n",
    "\n",
    "        agents.append(agent)\n",
    "        paths.append(path)\n",
    "        agent_safety_circles.append(safety)\n",
    "\n",
    "    ax.set_xlim(-1, env.x_range + 1)\n",
    "    ax.set_ylim(-1, env.y_range + 1)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal')\n",
    "    title = ax.set_title(\"Step: 0\")\n",
    "\n",
    "    max_steps = max(len(agent_path) for agent_path in last_successful_path)\n",
    "\n",
    "    def update_obstacle_positions():\n",
    "        for obstacle in obstacle_data:\n",
    "            if obstacle['path_type'] == 'circle':\n",
    "                obstacle['angle'] += obstacle['speed']\n",
    "                obstacle['position'][0] = obstacle['center'][0] + obstacle['radius'] * np.cos(obstacle['angle'])\n",
    "                obstacle['position'][1] = obstacle['center'][1] + obstacle['radius'] * np.sin(obstacle['angle'])\n",
    "\n",
    "            elif obstacle['path_type'] == 'vertical':\n",
    "                obstacle['position'][1] += obstacle['speed'] * obstacle['direction']\n",
    "                if obstacle['position'][1] >= obstacle['y_max']:\n",
    "                    obstacle['direction'] = -1\n",
    "                elif obstacle['position'][1] <= obstacle['y_min']:\n",
    "                    obstacle['direction'] = 1\n",
    "\n",
    "            elif obstacle['path_type'] == 'horizontal':\n",
    "                obstacle['position'][0] += obstacle['speed'] * obstacle['direction']\n",
    "                if obstacle['position'][0] >= obstacle['x_max']:\n",
    "                    obstacle['direction'] = -1\n",
    "                elif obstacle['position'][0] <= obstacle['x_min']:\n",
    "                    obstacle['direction'] = 1\n",
    "\n",
    "            elif obstacle['path_type'] == 'figure_eight':\n",
    "                obstacle['angle'] += obstacle['speed']\n",
    "                obstacle['position'][0] = obstacle['center'][0] + obstacle['a'] * np.sin(obstacle['angle'])\n",
    "                obstacle['position'][1] = obstacle['center'][1] + obstacle['b'] * np.sin(obstacle['angle'] * 2)\n",
    "\n",
    "            elif obstacle['path_type'] == 'diagonal':\n",
    "                obstacle['position'][0] += obstacle['speed'] * obstacle['direction_x']\n",
    "                obstacle['position'][1] += obstacle['speed'] * obstacle['direction_y']\n",
    "\n",
    "                if obstacle['position'][0] >= obstacle['x_max']:\n",
    "                    obstacle['direction_x'] = -1\n",
    "                elif obstacle['position'][0] <= obstacle['x_min']:\n",
    "                    obstacle['direction_x'] = 1\n",
    "\n",
    "                if obstacle['position'][1] >= obstacle['y_max']:\n",
    "                    obstacle['direction_y'] = -1\n",
    "                elif obstacle['position'][1] <= obstacle['y_min']:\n",
    "                    obstacle['direction_y'] = 1\n",
    "\n",
    "    def animate(frame):\n",
    "        update_obstacle_positions()\n",
    "\n",
    "        for i, (obs, safety_circle) in enumerate(zip(moving_obstacles, safety_circles)):\n",
    "            obs_pos = obstacle_data[i]['position']\n",
    "            obs.set_data([obs_pos[0]], [obs_pos[1]])\n",
    "            safety_circle.center = obs_pos\n",
    "\n",
    "        for i, (agent, path, safety_circle) in enumerate(zip(agents, paths, agent_safety_circles)):\n",
    "            if frame < len(last_successful_path[i]):\n",
    "                current_pos = last_successful_path[i][frame]\n",
    "            else:\n",
    "                current_pos = last_successful_path[i][-1]\n",
    "\n",
    "            agent.set_data([current_pos[0]], [current_pos[1]])\n",
    "\n",
    "            path_x = [pos[0] for pos in last_successful_path[i][:frame+1]]\n",
    "            path_y = [pos[1] for pos in last_successful_path[i][:frame+1]]\n",
    "            path.set_data(path_x, path_y)\n",
    "\n",
    "            safety_circle.center = current_pos\n",
    "\n",
    "        title.set_text(f\"Step: {frame}\")\n",
    "\n",
    "        return ([title] + moving_obstacles + safety_circles + agents + paths + agent_safety_circles)\n",
    "\n",
    "    anim = FuncAnimation(fig, animate, frames=max_steps, interval=100, blit=False, repeat=True)\n",
    "\n",
    "    anim.save('dynamic_obstacles_path.gif', writer='pillow')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    last_successful_path, rewards, env = train_multi_agent_ppo(num_episodes=1500)\n",
    "\n",
    "    if last_successful_path:\n",
    "        animate_last_successful_path(env, last_successful_path)\n",
    "    else:\n",
    "        print(\"No successful paths found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b4fd2",
   "metadata": {},
   "source": [
    "# FL COMBINED WITH BLOCKCHAIN FOR VERIFICATION(GANACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926787d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import inspect\n",
    "if not hasattr(inspect, 'getargspec'):\n",
    "    inspect.getargspec = inspect.getfullargspec\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import hashlib\n",
    "import json\n",
    "from web3 import Web3\n",
    "import os\n",
    "import json\n",
    "\n",
    "w3 = Web3(Web3.HTTPProvider(\"http://127.0.0.1:7545\"))  \n",
    "\n",
    "def load_contract():\n",
    "    with open('chain/build/contracts/FederatedLearningContract.json') as f:\n",
    "        contract_json = json.load(f)\n",
    "\n",
    "    network_id = w3.eth.chain_id\n",
    "    contract_address = \"0xC2E962d33E95f2cce8d0EB493025d4215220712B\"\n",
    "    contract_abi = contract_json['abi']\n",
    "    contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "    return contract\n",
    "\n",
    "contract = load_contract()\n",
    "\n",
    "def generate_model_hash(model):\n",
    "    params = []\n",
    "    for name, param in model.policy.named_parameters():\n",
    "        params.append(param.data.cpu().numpy().tolist())\n",
    "\n",
    "    model_json = json.dumps(params)\n",
    "    hash_bytes = Web3.keccak(text=model_json)\n",
    "    hash_hex = hash_bytes.hex()  \n",
    "\n",
    "    return hash_hex\n",
    "\n",
    "def check_contract_state():\n",
    "    current_round = contract.functions.currentRound().call()\n",
    "    aggregation_complete = contract.functions.aggregationComplete().call()\n",
    "\n",
    "    if current_round > 0:\n",
    "        for r in range(current_round):\n",
    "            try:\n",
    "                global_hash = contract.functions.globalModelHashes(r).call()\n",
    "                if isinstance(global_hash, bytes):\n",
    "                    print(f\"  Global Model Hash (Round {r}): 0x{global_hash.hex()[:10]}...\")\n",
    "                else:\n",
    "                    print(f\"  Global Model Hash (Round {r}): {global_hash[:10]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error getting global model hash for round {r}: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            global_hash = contract.functions.globalModelHashes(0).call()\n",
    "            if isinstance(global_hash, bytes):\n",
    "                print(f\"  Global Model Hash (Round 0): 0x{global_hash.hex()[:10]}...\")\n",
    "            else:\n",
    "                print(f\"  Global Model Hash (Round 0): {global_hash[:10]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error getting global model hash for round 0: {e}\")\n",
    "\n",
    "class BlockchainFederatedServer:\n",
    "    def __init__(self, state_dim, action_dim, num_clients):\n",
    "        self.global_model = PPOAgent(state_dim, action_dim)\n",
    "        self.num_clients = num_clients\n",
    "        self.ema_alpha = 0.8  \n",
    "        self.previous_global_state = None\n",
    "        self.momentum = 0.9\n",
    "        self.velocity = None\n",
    "        self.client_trust_scores = np.ones(num_clients)  \n",
    "        self.training_round = 0\n",
    "        self.current_round = 0  \n",
    "\n",
    "        self.server_account = w3.eth.accounts[0]\n",
    "\n",
    "        for i in range(1, num_clients + 1):\n",
    "            client_addr = w3.eth.accounts[i]\n",
    "            self.register_client(client_addr)\n",
    "\n",
    "    def register_client(self, client_address):\n",
    "        tx = contract.functions.registerClient(client_address).transact({\n",
    "            'from': self.server_account,\n",
    "            'gas': 2000000\n",
    "        })\n",
    "\n",
    "        receipt = w3.eth.waitForTransactionReceipt(tx)\n",
    "        print(f\"Client {client_address} registered. Transaction hash: {tx.hex()}\")\n",
    "\n",
    "    def start_new_round(self):\n",
    "        if not contract.functions.aggregationComplete().call():\n",
    "            print(\"Cannot start new round: Previous round's aggregation is not complete\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            tx = contract.functions.startNewRound().transact({\n",
    "                'from': self.server_account,\n",
    "                'gas': 2000000\n",
    "            })\n",
    "            receipt = w3.eth.waitForTransactionReceipt(tx)\n",
    "            \n",
    "            if receipt.status == 1:\n",
    "                self.current_round = contract.functions.currentRound().call()\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Transaction to start new round failed\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error starting new round: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def submit_global_model(self):\n",
    "        model_hash_hex = generate_model_hash(self.global_model)  \n",
    "        model_hash_bytes = Web3.toBytes(hexstr=model_hash_hex)\n",
    "\n",
    "        tx = contract.functions.submitGlobalModel(model_hash_bytes).transact({\n",
    "            'from': self.server_account,\n",
    "            'gas': 2000000\n",
    "        })\n",
    "\n",
    "        receipt = w3.eth.waitForTransactionReceipt(tx)\n",
    "        if receipt.status == 1:\n",
    "            print(f\"Global model successfully submitted.\")\n",
    "            stored_hash = contract.functions.globalModelHashes(self.current_round).call()\n",
    "            if isinstance(stored_hash, bytes):\n",
    "                print(f\"Verified stored hash: 0x{stored_hash.hex()[:10]}...\")\n",
    "            else:\n",
    "                print(f\"Verified stored hash: {stored_hash[:10]}...\")\n",
    "        else:\n",
    "            print(f\"Transaction failed when submitting global model!\")\n",
    "\n",
    "        return model_hash_bytes  \n",
    "\n",
    "\n",
    "    def verify_client_submissions(self):\n",
    "        all_updated = True\n",
    "        for i in range(1, self.num_clients + 1):\n",
    "            client_addr = w3.eth.accounts[i]\n",
    "            has_updated = contract.functions.hasUpdated(client_addr).call()\n",
    "            if not has_updated:\n",
    "                all_updated = False\n",
    "                print(f\"Client {client_addr} has not submitted an update\")\n",
    "\n",
    "        return all_updated and contract.functions.aggregationComplete().call()\n",
    "\n",
    "    def aggregate_weights(self, client_models, client_rewards, client_training_sizes=None, client_update_times=None):\n",
    "        self.training_round += 1\n",
    "        if self.training_round == 1:\n",
    "            max_reward = max(client_rewards)\n",
    "            self.client_trust_scores = [\n",
    "                1.0 + (reward / max_reward - 0.5) \n",
    "                for reward in client_rewards\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # Calculate model divergence\n",
    "            divergences = []\n",
    "            for i, client in enumerate(client_models):\n",
    "                # Compute divergence between client model and global model\n",
    "                divergence = 0\n",
    "                for key in client.policy.state_dict().keys():\n",
    "                    if 'running_mean' not in key and 'running_var' not in key:\n",
    "                        divergence += torch.norm(\n",
    "                            client.policy.state_dict()[key] - \n",
    "                            self.global_model.policy.state_dict()[key]\n",
    "                        ).item()\n",
    "                divergences.append(divergence)\n",
    "\n",
    "            # Normalize divergence\n",
    "            if max(divergences) > 0:\n",
    "                norm_divergences = [div / max(divergences) for div in divergences]\n",
    "                \n",
    "                # Update trust scores based on multiple factors\n",
    "                for i in range(self.num_clients):\n",
    "                    # Reward contribution (higher reward increases trust)\n",
    "                    reward_factor = client_rewards[i] / max(client_rewards) if max(client_rewards) > 0 else 1\n",
    "                    \n",
    "                    # Divergence contribution (less divergence increases trust)\n",
    "                    divergence_factor = 1 - norm_divergences[i]\n",
    "                    \n",
    "                    # Exponential moving average update\n",
    "                    self.client_trust_scores[i] = (\n",
    "                        0.7 * self.client_trust_scores[i] +  # Previous trust\n",
    "                        0.2 * reward_factor +               # Reward contribution\n",
    "                        0.1 * divergence_factor             # Model similarity\n",
    "                    )\n",
    "                    \n",
    "                    # Clip trust scores to a reasonable range\n",
    "                    self.client_trust_scores[i] = max(0.5, min(1.5, self.client_trust_scores[i]))\n",
    "\n",
    "        # Calculate weights considering trust scores\n",
    "        if sum(client_rewards) == 0:\n",
    "            base_weights = [1.0 / self.num_clients] * self.num_clients\n",
    "        else:\n",
    "            # Normalize rewards\n",
    "            min_reward = min(client_rewards)\n",
    "            shifted_rewards = [r - min_reward + 1e-5 for r in client_rewards] if min_reward < 0 else client_rewards\n",
    "            total_reward = sum(shifted_rewards)\n",
    "            base_weights = [r / total_reward for r in shifted_rewards]\n",
    "\n",
    "        # Apply trust scores to weights\n",
    "        weights = [w * trust for w, trust in zip(base_weights, self.client_trust_scores)]\n",
    "        total_weight = sum(weights)\n",
    "        weights = [w / total_weight for w in weights]\n",
    "\n",
    "        print(f\"Round {self.training_round - 1} Analysis:\")\n",
    "        print(f\"  Rewards:        {[f'{r:.2f}' for r in client_rewards]}\")\n",
    "        print(f\"  Trust Scores:   {[f'{t:.2f}' for t in self.client_trust_scores]}\")\n",
    "        print(f\"  Aggregation Weights: {[f'{w:.3f}' for w in weights]}\")\n",
    "\n",
    "        new_state_dict = deepcopy(client_models[0].policy.state_dict())\n",
    "        for key in new_state_dict.keys():\n",
    "            if 'running_mean' in key or 'running_var' in key:\n",
    "                continue\n",
    "            new_state_dict[key] = sum(weights[i] * client_models[i].policy.state_dict()[key]\n",
    "                                    for i in range(self.num_clients))\n",
    "\n",
    "        self.previous_global_state = deepcopy(new_state_dict)\n",
    "        self.global_model.policy.load_state_dict(new_state_dict)\n",
    "\n",
    "        self.submit_global_model()\n",
    "class SecurityError(Exception):\n",
    "        pass\n",
    "\n",
    "class BlockchainFederatedClient:\n",
    "    def __init__(self, state_dim, action_dim, env, agent_id, account_index):\n",
    "        self.model = PPOAgent(state_dim, action_dim)\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.best_path = None\n",
    "        self.best_reward = -float('inf')\n",
    "        self.training_size = 0\n",
    "        self.last_update_time = 0\n",
    "        self.account_address = w3.eth.accounts[account_index]\n",
    "\n",
    "    def submit_update_to_blockchain(self, reward):\n",
    "        has_updated = contract.functions.hasUpdated(self.account_address).call()\n",
    "\n",
    "        if has_updated:\n",
    "            print(f\"Client {self.agent_id} has already submitted an update for this round.\")\n",
    "            return False, None\n",
    "\n",
    "        print(f\"Verifying client {self.agent_id} with account {self.account_address} before submitting update...\")\n",
    "        \n",
    "        is_authorized = contract.functions.authorizedClients(self.account_address).call()\n",
    "        if not is_authorized:\n",
    "            print(f\"ERROR: Client {self.agent_id} is not authorized and cannot submit updates!\")\n",
    "            print(\"SECURITY ALERT: Unauthorized client detected - terminating federation process\")\n",
    "            print(\"===== FEDERATION PROCESS TERMINATED =====\")\n",
    "            sys.exit(1)\n",
    "            return False, None\n",
    "        else:\n",
    "            print(f\"Client {self.agent_id} verification successful - client is authorized in the contract.\")\n",
    "\n",
    "        model_hash = generate_model_hash(self.model) \n",
    "        reward_int = int(reward * 100)  \n",
    "\n",
    "        try:\n",
    "            tx = contract.functions.submitUpdate(Web3.toBytes(hexstr=model_hash), reward_int).transact({\n",
    "                'from': self.account_address,\n",
    "                'gas': 2000000\n",
    "            })\n",
    "\n",
    "            receipt = w3.eth.waitForTransactionReceipt(tx)\n",
    "\n",
    "            if receipt.status == 1:\n",
    "                print(f\"Client {self.agent_id} submitted update. Transaction hash: {tx.hex()}\")\n",
    "                print(f\"Update verified on-chain via the onlyAuthorizedClient modifier\")\n",
    "                return True, model_hash\n",
    "            else:\n",
    "                print(f\"Transaction failed for Client {self.agent_id}.\")\n",
    "                return False, None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting update: {str(e)}\")\n",
    "            if \"revert\" in str(e).lower():\n",
    "                print(f\"Transaction reverted - client likely failed the onlyAuthorizedClient verification\")\n",
    "            return False, None \n",
    "\n",
    "    def receive_global_model(self, global_model, round_num):\n",
    "        expected_hash = generate_model_hash(global_model)\n",
    "        if self.verify_global_model(expected_hash, round_num):\n",
    "            self.model.policy.load_state_dict(global_model.policy.state_dict())\n",
    "            print(f\"Client {self.agent_id} successfully received verified global model\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Client {self.agent_id} rejected unverified global model\")\n",
    "            return False\n",
    "\n",
    "    def train(self, num_episodes=10, round_num=0):\n",
    "        self.last_update_time = round_num\n",
    "        best_local_path = None\n",
    "        best_local_reward = -float('inf')\n",
    "        successful_paths = []\n",
    "        self.training_size = num_episodes  \n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            states = self.env.reset()\n",
    "            total_rewards = [0, 0]\n",
    "            paths = [[], []]\n",
    "            agent_trajectories = [\n",
    "                {'states': [], 'actions': [], 'log_probs': [], 'rewards': [],\n",
    "                 'next_states': [], 'dones': []}\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "\n",
    "            for step in range(self.env.max_steps):\n",
    "                actions, log_probs = [], []\n",
    "                for i in range(2):\n",
    "                    action, log_prob = self.model.select_action(states[i])\n",
    "                    actions.append(action)\n",
    "                    log_probs.append(log_prob)\n",
    "\n",
    "                next_states, rewards, dones, _ = self.env.step(actions)\n",
    "\n",
    "                for i in range(2):\n",
    "                    agent_trajectories[i]['states'].append(states[i])\n",
    "                    agent_trajectories[i]['actions'].append(actions[i])\n",
    "                    agent_trajectories[i]['log_probs'].append(log_probs[i])\n",
    "                    agent_trajectories[i]['rewards'].append(rewards[i])\n",
    "                    agent_trajectories[i]['next_states'].append(next_states[i])\n",
    "                    agent_trajectories[i]['dones'].append(dones[i])\n",
    "                    paths[i].append(self.env.agents[i]['position'])\n",
    "                    total_rewards[i] += rewards[i]\n",
    "\n",
    "                conflict = detect_conflict(paths)\n",
    "                if conflict:\n",
    "                    self.env.apply_cbs_constraints(conflict)\n",
    "\n",
    "                states = next_states\n",
    "                if all(dones):\n",
    "                    successful_paths.append((deepcopy(paths), sum(total_rewards)))\n",
    "                    break\n",
    "\n",
    "            combined_reward = sum(total_rewards)\n",
    "\n",
    "            if combined_reward > best_local_reward:\n",
    "                best_local_reward = combined_reward\n",
    "                best_local_path = deepcopy(paths)\n",
    "\n",
    "            for i in range(2):\n",
    "                self.model.update(\n",
    "                    agent_trajectories[i]['states'],\n",
    "                    agent_trajectories[i]['actions'],\n",
    "                    agent_trajectories[i]['log_probs'],\n",
    "                    agent_trajectories[i]['rewards'],\n",
    "                    agent_trajectories[i]['next_states'],\n",
    "                    agent_trajectories[i]['dones']\n",
    "                )\n",
    "\n",
    "        if successful_paths:\n",
    "            self.best_path = max(successful_paths, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            self.best_path = best_local_path\n",
    "\n",
    "        self.best_reward = best_local_reward\n",
    "        submission_result, model_hash = self.submit_update_to_blockchain(self.best_reward)\n",
    "\n",
    "        return self.model, submission_result, model_hash\n",
    "\n",
    "    def verify_global_model(self, expected_hash, round_num):\n",
    "        on_chain_hash = contract.functions.globalModelHashes(round_num).call()\n",
    "        print(f\"Verification \")\n",
    "        print(f\"  Expected hash: {expected_hash[:10]}...\")\n",
    "        if isinstance(on_chain_hash, bytes):\n",
    "            on_chain_hash = '0x' + on_chain_hash.hex()\n",
    "        print(f\"  On-chain hash: {on_chain_hash[:10]}...\")\n",
    "\n",
    "        if on_chain_hash == expected_hash:\n",
    "            print(f\"  Global model verified\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  Global model verification failed\")\n",
    "            return False\n",
    "def test_unregistered_client():\n",
    "    unregistered_account = w3.eth.accounts[5]\n",
    "    print(f\"\\n=== Testing unregistered client with account {unregistered_account} ===\")\n",
    "    unregistered_client = BlockchainFederatedClient(state_dim, action_dim, MultiAgentEnv(), 999, 5)\n",
    "    print(\"Training unregistered client...\")\n",
    "    unregistered_client.train(num_episodes=10, round_num=0)\n",
    "\n",
    "sample_env = MultiAgentEnv()\n",
    "sample_state = sample_env.reset()\n",
    "state_dim = len(sample_state[0])\n",
    "action_dim = 8\n",
    "no_rounds = 5\n",
    "num_episodes = 100\n",
    "\n",
    "num_clients = 2\n",
    "envs = [MultiAgentEnv() for _ in range(num_clients)]\n",
    "server = BlockchainFederatedServer(state_dim, action_dim, num_clients)\n",
    "clients = [\n",
    "    BlockchainFederatedClient(state_dim, action_dim, envs[i], i, i+1)  \n",
    "    for i in range(num_clients)\n",
    "]\n",
    "\n",
    "rewards_history = []\n",
    "client_rewards_history = [[] for _ in range(num_clients)]\n",
    "\n",
    "check_contract_state()\n",
    "# For checking against random agent which is not registered\n",
    "# test_unregistered_client() \n",
    "for round in range(no_rounds):\n",
    "    print(f\"\\n==== STARTING TRAINING ROUND {round}====\\n\")\n",
    "    \n",
    "    if round > 0:\n",
    "        server.start_new_round()\n",
    "        current_round = contract.functions.currentRound().call()\n",
    "    else:\n",
    "        current_round = contract.functions.currentRound().call()\n",
    "\n",
    "    client_models = []\n",
    "    round_rewards = []\n",
    "    training_sizes = []\n",
    "    update_times = []\n",
    "\n",
    "    for client in clients:\n",
    "        print(f\"Client {client.agent_id} training in round {round}...\")\n",
    "        model, submission_success, _ = client.train(num_episodes, current_round)\n",
    "        if not submission_success:\n",
    "            print(f\"Client {client.agent_id} failed to submit update. Terminating federation process.\")\n",
    "            print(\"===== FEDERATION PROCESS TERMINATED =====\")\n",
    "            sys.exit(1) \n",
    "        else:\n",
    "            client_models.append(model)\n",
    "            round_rewards.append(client.best_reward)\n",
    "            client_rewards_history[client.agent_id].append(client.best_reward)\n",
    "            training_sizes.append(client.training_size)\n",
    "            update_times.append(client.last_update_time)\n",
    "\n",
    "    check_contract_state()\n",
    "    print(\"\\nAggregating client models...\")\n",
    "    server.aggregate_weights(\n",
    "        client_models,\n",
    "        round_rewards,\n",
    "        training_sizes,\n",
    "        update_times\n",
    "    )\n",
    "\n",
    "    check_contract_state()\n",
    "    print(\"\\nDistributing global model to clients...\")\n",
    "    verification_success = True\n",
    "    for client in clients:\n",
    "        if not client.receive_global_model(server.global_model, current_round):\n",
    "            verification_success = False\n",
    "            print(f\"Verification failed for client {client.agent_id}, stopping federation process\")\n",
    "            sys.exit(1)\n",
    "            break\n",
    "\n",
    "    if not verification_success:\n",
    "        print(\"Cannot proceed to next round due to verification failure\")\n",
    "        break  \n",
    "\n",
    "    rewards_history.append(np.mean(round_rewards))\n",
    "    print(f\"Round {round}: Avg Reward = {np.mean(round_rewards):.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards_history, 'k-', linewidth=2, label=\"Global Avg Reward\")\n",
    "for i in range(num_clients):\n",
    "    plt.plot(client_rewards_history[i], linestyle='--', marker='o', label=f\"Client {i} Reward\")\n",
    "plt.xlabel(\"Rounds\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Blockchain-Enhanced Federated Learning Training Progress\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "last_successful_path = None\n",
    "for client in clients:\n",
    "    if client.best_path:\n",
    "        last_successful_path = client.best_path\n",
    "        break\n",
    "\n",
    "if last_successful_path:\n",
    "    animate_last_successful_path(envs[0], last_successful_path)\n",
    "else:\n",
    "    print(\"No successful paths found\")\n",
    "\n",
    "print(\"Blockchain-enhanced federated training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
